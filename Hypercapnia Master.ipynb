{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Run Entire Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows vs Mac working directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mac\n",
    "#notebook_dir = r\"/Users/blocke/TriNetX Working/Notebooks\"\n",
    "\n",
    "# Windows\n",
    "#notebook_dir = r\"C:\\Users\\reblo\\TriNetX Preprocessing\\Notebooks\"\n",
    "notebook_dir = r\"C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from nbconvert import NotebookExporter\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to execute a notebook and save the output\n",
    "def execute_notebook(notebook_path):\n",
    "    # Load the notebook\n",
    "    with open(notebook_path) as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "        \n",
    "    # Create an ExecutePreprocessor instance\n",
    "    ep = ExecutePreprocessor(timeout=-1, kernel_name='python3')\n",
    "    \n",
    "    # Execute the notebook\n",
    "    try:\n",
    "        ep.preprocess(nb, {'metadata': {'path': './'}})\n",
    "        print(f\"Successfully executed {notebook_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing {notebook_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Save the notebook with the executed cells\n",
    "    with open(notebook_path, 'wt') as f:\n",
    "        nbformat.write(nb, f)\n",
    "\n",
    "\n",
    "\n",
    "def execute_notebook_with_variables(notebook_path, output_path, variables_dict):\n",
    "    # Load the notebook\n",
    "    with open(notebook_path) as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    # Create a new code cell to define the variables\n",
    "    code = '\\n'.join(f\"{key} = {repr(value)}\" for key, value in variables_dict.items())\n",
    "    new_cell = nbformat.v4.new_code_cell(source=code)\n",
    "    \n",
    "    # Insert the new cell at the beginning of the notebook\n",
    "    nb['cells'].insert(0, new_cell)\n",
    "    \n",
    "    # Create an ExecutePreprocessor instance\n",
    "    ep = ExecutePreprocessor(timeout=-1, kernel_name='python3') \n",
    "    # Execute the notebook\n",
    "    try:\n",
    "        ep.preprocess(nb, {'metadata': {'path': './'}})\n",
    "        print(f\"Successfully executed {notebook_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing {notebook_path}: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Save the executed notebook\n",
    "    with open(output_path, 'wt') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    print(f\"Notebook executed and saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "print(os.path.join(notebook_dir, r\"Hypercapnia NEW DATA - Encounter (CSV Processing).ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia NEW DATA - Encounter (CSV Processing).ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior diagnosis\n",
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia NEW DATA - Prior Diagnosis (CSV Processing).ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Diagnosis\n",
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia NEW DATA - Current Diagnosis (CSV Processing).ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medications\n",
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia NEW DATA - Medication (CSV Processing).ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vital Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vital Signs\n",
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia NEW DATA - Vital Signs (CSV Processing).ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procedures \n",
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia NEW DATA - Procedure (CSV Processing).ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia NEW DATA - Lab Results (CSV Processing).ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia NEW DATA - RFS Processing.ipynb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS ABG, EMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_ABG_ENC_EMER - Legacy.ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS ABG, INP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_ABG_ENC_INPAT - Legacy.ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS ABG, AMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_notebook(os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_ABG_ENC_AMB - Legacy.ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data Check Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Check Call. All RFS/Permutations must have been generated, \n",
    "# and this must be done before any data checks on the below calls have been performed\n",
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Data Checks.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Data Checks - Executed.ipynb\")\n",
    "variables_dict = {'database_dir': r\"E:\\TriNetX\\\\\", 'working_dir': r\"C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\\\\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS OBESITY, AMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\zmq\\_future.py:679: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_OBESITY_ENC_AMB.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_OBESITY_ENC_AMB.ipynb\")\n",
    "variables_dict = {'output_dir': \"AMBULATORY\", 'setting': \"AMB\", 'rfs':\"OBESITY\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS OBESITY, EMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_OBESITY_ENC_EMER.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_OBESITY_ENC_EMER.ipynb\")\n",
    "variables_dict = {'output_dir': \"EMERGENCY\", 'setting': \"EMER\", 'rfs':\"OBESITY\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS OBESITY, INP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_OBESITY_ENC_INPAT.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_OBESITY_ENC_INPAT.ipynb\")\n",
    "variables_dict = {'output_dir': \"INPATIENT\", 'setting': \"INPAT\", 'rfs':\"OBESITY\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS PREDISPOSITION, AMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb: An error occurred while executing the following cell:\n",
      "------------------\n",
      "start_time = time.time()\n",
      "RFS_ABG_ENC_AMB_BEFORE = pd.read_csv(os.path.join(working_dir[:-1], \"output\", output_dir, \"RFS_\"+rfs+\"_ENC_\"+setting+\"_BEFORE.csv\"))\n",
      "\n",
      "\"\"\"\n",
      "num_spreadsheets = 10\n",
      "#num_spreadsheets = 853\n",
      "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"date\",\"value\",\"text_value\",\"units_of_measure\",\"derived_by_TriNetX\",\"source_id\"]\n",
      "unique_chunks = []\n",
      "for i in range(1, num_spreadsheets+1):\n",
      "    print(f'{i:04}')\n",
      "    vital_signs = pd.read_csv(database_dir + r\"Vital Signs\\vital_signs\"+f'{i:04}'+\".csv\",\n",
      "                            names = columns,\n",
      "                            usecols = [\"encounter_id\"],\n",
      "                            dtype = {\"encounter_id\":str})\n",
      "    unique_chunk = vital_signs[\"encounter_id\"].unique().tolist()\n",
      "    unique_chunks.append(unique_chunk)\n",
      "    del unique_chunk\n",
      "unique_vs_encounters = list(set([item for sublist in unique_chunks for item in sublist]))\n",
      "\"\"\"\n",
      "#unique_vs_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"vitals_encounters.csv\"))[\"encounter_id\"])\n",
      "#print(\"Vitals\", len(unique_vs_encounters))\n",
      "\n",
      "# Current Diagnosis\n",
      "\"\"\"\n",
      "#num_spreadsheets = 1273\n",
      "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
      "unique_chunks = []\n",
      "for i in range(1, num_spreadsheets+1):\n",
      "    print(f'{i:04}')\n",
      "    vital_signs = pd.read_csv(database_dir + r\"Diagnosis\\diagnosis\"+f'{i:04}'+\".csv\",\n",
      "                            names = columns,\n",
      "                            usecols = [\"encounter_id\"],\n",
      "                            dtype = {\"encounter_id\":str})\n",
      "    unique_chunk = vital_signs[\"encounter_id\"].unique().tolist()\n",
      "    unique_chunks.append(unique_chunk)\n",
      "    del unique_chunk\n",
      "unique_diag_encounters = list(set([item for sublist in unique_chunks for item in sublist]))\n",
      "\"\"\"\n",
      "#unique_diag_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"diagnosis_encounters.csv\"))[\"encounter_id\"])\n",
      "#print(\"Diagnosis \", len(unique_diag_encounters))\n",
      "\n",
      "# Print the number of unique patients who passed the data quality check\n",
      "\n",
      "# Filter out patients who did not pass the data quality check\n",
      "if setting == \"AMB\":\n",
      "    new_OP_data_quality_check_FINAL_patients = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"amb_enc_screen.csv\"))[\"encounter_id\"])\n",
      "    print(\"Number of Unique Encounters in OP Filter:\", len(new_OP_data_quality_check_FINAL_patients))\n",
      "    print(\"Prefilter: \")\n",
      "    print(RFS_ABG_ENC_AMB_BEFORE.shape)\n",
      "    RFS_ABG_ENC_AMB_BEFORE = RFS_ABG_ENC_AMB_BEFORE[RFS_ABG_ENC_AMB_BEFORE[\"encounter_id\"].isin(new_OP_data_quality_check_FINAL_patients)]\n",
      "else: \n",
      "    # Lab Results\n",
      "    \"\"\"\n",
      "    num_spreadsheets = 2334\n",
      "    columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"date\",\"value\",\"text_value\",\"units_of_measure\",\"derived_by_TriNetX\",\"source_id\"]\n",
      "    unique_chunks = []\n",
      "    for i in range(1, num_spreadsheets+1):\n",
      "        print(f'{i:04}')\n",
      "        vital_signs = pd.read_csv(database_dir + r\"Lab Results\\lab_results\"+f'{i:04}'+\".csv\",\n",
      "                                names = columns,\n",
      "                                usecols = [\"encounter_id\"],\n",
      "                                dtype = {\"encounter_id\":str})\n",
      "        unique_chunk = vital_signs[\"encounter_id\"].unique().tolist()\n",
      "        unique_chunks.append(unique_chunk)\n",
      "        del unique_chunk\n",
      "    unique_lab_encounters = list(set([item for sublist in unique_chunks for item in sublist]))\n",
      "    \"\"\"\n",
      "    #unique_lab_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"labs_encounters.csv\"))[\"encounter_id\"])\n",
      "    #print(\"Lab\", len(unique_lab_encounters))\n",
      "\n",
      "    # Current Procedure\n",
      "    \"\"\"\n",
      "    num_spreadsheets = 714\n",
      "    columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_procedure_indicator\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
      "    unique_chunks = []\n",
      "    for i in range(1, num_spreadsheets+1):\n",
      "        print(f'{i:04}')\n",
      "        vital_signs = pd.read_csv(database_dir + r\"Procedure\\procedure\"+f'{i:04}'+\".csv\",\n",
      "                                names = columns,\n",
      "                                usecols = [\"encounter_id\"],\n",
      "                                dtype = {\"encounter_id\":str})\n",
      "        unique_chunk = vital_signs[\"encounter_id\"].unique().tolist()\n",
      "        unique_chunks.append(unique_chunk)\n",
      "        del unique_chunk\n",
      "    unique_proc_encounters = list(set([item for sublist in unique_chunks for item in sublist]))\n",
      "    \"\"\"\n",
      "    #unique_proc_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"procedure_encounters.csv\"))[\"encounter_id\"])\n",
      "    #print(\"Procedure \", len(unique_proc_encounters))\n",
      "\n",
      "    # Inpatient Medication\n",
      "    \"\"\"\n",
      "    num_spreadsheets = 2991\n",
      "    columns = [\"patient_id\",\"encounter_id\",\"unique_id\",\"code_system\",\"code\",\"start_date\",\"route\",\"brand\",\"strength\",\"derived_by_TriNetX\",\"source_id\"]\n",
      "    unique_chunks = []\n",
      "    for i in range(1, num_spreadsheets+1):\n",
      "        print(f'{i:04}')\n",
      "        vital_signs = pd.read_csv(database_dir + r\"Medications\\medication\"+f'{i:04}'+\".csv\",\n",
      "                                names = columns,\n",
      "                                usecols = [\"encounter_id\"],\n",
      "                                dtype = {\"encounter_id\":str})\n",
      "        unique_chunk = vital_signs[\"encounter_id\"].unique().tolist()\n",
      "        unique_chunks.append(unique_chunk)\n",
      "        del unique_chunk\n",
      "    unique_med_encounters = list(set([item for sublist in unique_chunks for item in sublist]))\n",
      "    \"\"\"\n",
      "    #unique_med_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"medication_encounters.csv\"))[\"encounter_id\"])\n",
      "    #print(\"Med \", len(unique_med_encounters))\n",
      "    new_IP_data_quality_check_FINAL_patients = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"inp_enc_screen.csv\"))[\"encounter_id\"])\n",
      "\n",
      "    #new_IP_data_quality_check_FINAL_patients = list(set(unique_vs_encounters) & set(unique_diag_encounters) & set(unique_med_encounters) & set(unique_proc_encounters) & set(unique_lab_encounters))\n",
      "    print(\"Number of Unique Encounters in IP Filter:\", len(new_IP_data_quality_check_FINAL_patients))\n",
      "    print(\"Prefilter: \")\n",
      "    print(RFS_ABG_ENC_AMB_BEFORE.shape)\n",
      "    RFS_ABG_ENC_AMB_BEFORE = RFS_ABG_ENC_AMB_BEFORE[RFS_ABG_ENC_AMB_BEFORE[\"encounter_id\"].isin(new_IP_data_quality_check_FINAL_patients)]\n",
      "\n",
      "print(\"\\n\")\n",
      "display(RFS_ABG_ENC_AMB_BEFORE.head())\n",
      "print(RFS_ABG_ENC_AMB_BEFORE.shape)\n",
      "print(RFS_ABG_ENC_AMB_BEFORE.isna().sum().sum())\n",
      "\n",
      "RFS_ABG_ENC_AMB_BEFORE.to_csv(os.path.join(working_dir[:-1], \"output\", output_dir, \"RFS_\"+rfs+\"_ENC_\"+setting+\"_AFTER_NEW.csv\"), index = False) \n",
      "\n",
      "end_time = time.time()\n",
      "execution_time = end_time - start_time\n",
      "hours = int(execution_time // 3600)\n",
      "minutes = int((execution_time % 3600) // 60)\n",
      "seconds = execution_time % 60\n",
      "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
      "gc.collect()\n",
      "\n",
      "------------------\n",
      "\n",
      "----- stderr -----\n",
      "C:\\Users\\reblo\\AppData\\Local\\Temp\\ipykernel_5224\\1149836878.py:2: DtypeWarning: Columns (12,14,16,18,20,22,24,26,32,34,36,38,41,43,45,46,48,50,52,54,56,58,60,62,64,66,68,70,72,84,88,90,92,94,96,98,100,102,104,106,108,114,116,128,133,138,143,148,150,151,152,153,155,159,161,165,167,169,171,175,179,183,187,189,191,193,195,197,199,203,205,209,211,213,215,217,219,221,223,225,227,229,231,233,235,239,243,245,247,249,251,253,255,257,259,261,267,269,271,273,275,279,280,282,283,285,286,288,289,291,292,294,295,297,298,300,301,303,304,318,319,321,322,324,325,327,328,330,331,336,337,339,340,342,343,345,346,348,349,351,352,354,355,357,358,381,382,387,388,390,391,393,394,396,397,399,400,402,403,405,406,408,409,411,412,417,418,420,421,423,424,426,427,432,433,438,439,441,442,450,451,459,460,461,462,467,468,470,471,473,474,476,477,482,483,488,489,491,492,497,498,500,501,503,505,509,511,513,515) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  RFS_ABG_ENC_AMB_BEFORE = pd.read_csv(os.path.join(working_dir[:-1], \"output\", output_dir, \"RFS_\"+rfs+\"_ENC_\"+setting+\"_BEFORE.csv\"))\n",
      "------------------\n",
      "\n",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;32m----> 2\u001b[0m RFS_ABG_ENC_AMB_BEFORE \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(working_dir[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRFS_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mrfs\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ENC_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msetting\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_BEFORE.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mnum_spreadsheets = 10\u001b[39;00m\n",
      "\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m#num_spreadsheets = 853\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03munique_vs_encounters = list(set([item for sublist in unique_chunks for item in sublist]))\u001b[39;00m\n",
      "\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#unique_vs_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"vitals_encounters.csv\"))[\"encounter_id\"])\u001b[39;00m\n",
      "\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#print(\"Vitals\", len(unique_vs_encounters))\u001b[39;00m\n",
      "\u001b[0;32m     22\u001b[0m \n",
      "\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Current Diagnosis\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n",
      "\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n",
      "\u001b[0;32m    936\u001b[0m     dialect,\n",
      "\u001b[0;32m    937\u001b[0m     delimiter,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n",
      "\u001b[0;32m    945\u001b[0m )\n",
      "\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n",
      "\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n",
      "\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n",
      "\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1765\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n",
      "\u001b[0;32m   1762\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m   1763\u001b[0m         new_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index)\n",
      "\u001b[1;32m-> 1765\u001b[0m     df \u001b[38;5;241m=\u001b[39m DataFrame(col_dict, columns\u001b[38;5;241m=\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39mindex)\n",
      "\u001b[0;32m   1767\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n",
      "\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n",
      "\u001b[0;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n",
      "\u001b[0;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n",
      "\u001b[0;32m    729\u001b[0m     )\n",
      "\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n",
      "\u001b[1;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n",
      "\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n",
      "\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n",
      "\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n",
      "\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n",
      "\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n",
      "\u001b[0;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n",
      "\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_block_manager_from_column_arrays(\n",
      "\u001b[0;32m    153\u001b[0m         arrays, axes, consolidate\u001b[38;5;241m=\u001b[39mconsolidate, refs\u001b[38;5;241m=\u001b[39mrefs\n",
      "\u001b[0;32m    154\u001b[0m     )\n",
      "\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2086\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n",
      "\u001b[0;32m   2068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_block_manager_from_column_arrays\u001b[39m(\n",
      "\u001b[0;32m   2069\u001b[0m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n",
      "\u001b[0;32m   2070\u001b[0m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   2082\u001b[0m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n",
      "\u001b[0;32m   2083\u001b[0m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n",
      "\u001b[0;32m   2085\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m-> 2086\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m _form_blocks(arrays, consolidate, refs)\n",
      "\u001b[0;32m   2087\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m BlockManager(blocks, axes, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;32m   2088\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2160\u001b[0m, in \u001b[0;36m_form_blocks\u001b[1;34m(arrays, consolidate, refs)\u001b[0m\n",
      "\u001b[0;32m   2157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype\u001b[38;5;241m.\u001b[39mtype, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n",
      "\u001b[0;32m   2158\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[1;32m-> 2160\u001b[0m values, placement \u001b[38;5;241m=\u001b[39m _stack_arrays(\u001b[38;5;28mlist\u001b[39m(tup_block), dtype)\n",
      "\u001b[0;32m   2161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n",
      "\u001b[0;32m   2162\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n",
      "\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2200\u001b[0m, in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n",
      "\u001b[0;32m   2197\u001b[0m first \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m   2198\u001b[0m shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(arrays),) \u001b[38;5;241m+\u001b[39m first\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;32m-> 2200\u001b[0m stacked \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;32m   2201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n",
      "\u001b[0;32m   2202\u001b[0m     stacked[i] \u001b[38;5;241m=\u001b[39m arr\n",
      "\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.88 MiB for an array with shape (1, 1294920) and data type int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this is the biggest one.\n",
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_PREDISPOSITION_ENC_AMB.ipynb\")\n",
    "variables_dict = {'output_dir': \"AMBULATORY\", 'setting': \"AMB\", 'rfs':\"PREDISPOSITION\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS PREDISPOSITION, EMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\zmq\\_future.py:679: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_PREDISPOSITION_ENC_EMER.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_PREDISPOSITION_ENC_EMER.ipynb\")\n",
    "variables_dict = {'output_dir': \"EMERGENCY\", 'setting': \"EMER\", 'rfs':\"PREDISPOSITION\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS PREDISPOSITION, INP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_PREDISPOSITION_ENC_INPAT.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_PREDISPOSITION_ENC_INPAT.ipynb\")\n",
    "variables_dict = {'output_dir': \"INPATIENT\", 'setting': \"INPAT\", 'rfs':\"PREDISPOSITION\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS RESPFAIL, AMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_RESPFAIL_ENC_AMB.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_RESPFAIL_ENC_AMB.ipynb\")\n",
    "variables_dict = {'output_dir': \"AMBULATORY\", 'setting': \"AMB\", 'rfs':\"RESPFAIL\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS RESPFAIL, EMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_RESPFAIL_ENC_EMER.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_RESPFAIL_ENC_EMER.ipynb\")\n",
    "variables_dict = {'output_dir': \"EMERGENCY\", 'setting': \"EMER\", 'rfs':\"RESPFAIL\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS RESPFAIL, INP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_RESPFAIL_ENC_INPAT.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\") \n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_RESPFAIL_ENC_INPAT.ipynb\")\n",
    "variables_dict = {'output_dir': \"INPATIENT\", 'setting': \"INPAT\", 'rfs':\"RESPFAIL\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS VBG, AMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_VBG_ENC_AMB.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_VBG_ENC_AMB.ipynb\")\n",
    "variables_dict = {'output_dir': \"AMBULATORY\", 'setting': \"AMB\", 'rfs':\"VBG\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS VBG, EMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_VBG_ENC_EMER.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_VBG_ENC_EMER.ipynb\")\n",
    "variables_dict = {'output_dir': \"EMERGENCY\", 'setting': \"EMER\", 'rfs':\"VBG\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS VBG, INP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_VBG_ENC_INPAT.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_VBG_ENC_INPAT.ipynb\")\n",
    "variables_dict = {'output_dir': \"INPATIENT\", 'setting': \"INPAT\", 'rfs':\"VBG\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS VENT, AMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_VENTSUPPORT_ENC_AMB.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_VENTSUPPORT_ENC_AMB.ipynb\")\n",
    "variables_dict = {'output_dir': \"AMBULATORY\", 'setting': \"AMB\", 'rfs':\"VENTSUPPORT\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS VENT, EMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_VENTSUPPORT_ENC_EMER.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_VENTSUPPORT_ENC_EMER.ipynb\")\n",
    "variables_dict = {'output_dir': \"EMERGENCY\", 'setting': \"EMER\", 'rfs':\"VENTSUPPORT\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS VENT, INP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_VENTSUPPORT_ENC_INPAT.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_VENTSUPPORT_ENC_INPAT.ipynb\")\n",
    "variables_dict = {'output_dir': \"INPATIENT\", 'setting': \"INPAT\", 'rfs':\"VENTSUPPORT\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS ABG, AMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_ABG_ENC_AMB2.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_ABG_ENC_AMB2.ipynb\")\n",
    "variables_dict = {'output_dir': \"AMBULATORY\", 'setting': \"AMB\", 'rfs':\"ABG\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS ABG, EMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_ABG_ENC_EMER2.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_ABG_ENC_EMER2.ipynb\")\n",
    "variables_dict = {'output_dir': \"EMERGENCY\", 'setting': \"EMER\", 'rfs':\"ABG\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS AMB, INPAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - Master.ipynb\n",
      "Notebook executed and saved to C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\trinetx-preprocessing\\Hypercapnia Final Dataset Generation - RFS_ABG_ENC_INPAT2.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - Master.ipynb\")\n",
    "output_path = os.path.join(notebook_dir, r\"Hypercapnia Final Dataset Generation - RFS_ABG_ENC_INPAT2.ipynb\")\n",
    "variables_dict = {'output_dir': \"INPATIENT\", 'setting': \"INPAT\", 'rfs':\"ABG\"}\n",
    "execute_notebook_with_variables(notebook_path, output_path, variables_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
