{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3921cd09",
   "metadata": {},
   "source": [
    "# Encounter Preprocessing (NEW DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "database_dir = \"D:\\\\TriNetX\\\\Encounter\\\\\"  # Location where the database files are stored \n",
    "#working_dir = \"D:\\\\TriNetX\\\\Encounter\\\\\"\n",
    "working_dir = r\"C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\\\\" #location where to read and right from (faster = better if space allows)\n",
    "num_spreadsheets = 593"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d4d44",
   "metadata": {},
   "source": [
    "### Reformatting of CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf56cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working_dir = \"D:\\\\TriNetX\\\\Encounter\\\\\"\n",
    "#num_spreadsheets = 100\n",
    "\n",
    "columns = [\"encounter_id\",\"patient_id\",\"start_date\",\"end_date\",\"type\",\"start_date_derived_by_TriNetX\",\"end_date_derived_by_TriNetX\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "for i in range(1, num_spreadsheets+1):\n",
    "    print(f'{i:04}')\n",
    "    encounter = pd.read_csv(database_dir + \"encounter\"+f'{i:04}'+\".csv\",\n",
    "                         names = columns,\n",
    "                         dtype = {\"encounter_id\":str,\"patient_id\":str,\"type\":str,\"start_date_derived_by_TriNetX\":str,\"end_date_derived_by_TriNetX\":str,\"derived_by_TriNetX\":str,\"source_id\":str},\n",
    "                         parse_dates = [\"start_date\",\"end_date\"])\n",
    "    encounter = encounter.drop(columns = [\"start_date_derived_by_TriNetX\",\"end_date_derived_by_TriNetX\",\"derived_by_TriNetX\",\"source_id\"])\n",
    "    encounter = encounter[[\"patient_id\", \"encounter_id\", \"start_date\", \"end_date\", \"type\"]]\n",
    "    encounter = encounter[encounter[\"type\"].str.match(\"^AMB$|^EMER$|^IMP$\")]\n",
    "    encounter[\"start_date\"] = pd.to_datetime(encounter[\"start_date\"])\n",
    "    encounter[\"end_date\"] = pd.to_datetime(encounter[\"end_date\"])\n",
    "    encounter.to_csv(working_dir + \"encounter_NEW_\"+f'{i:04}'+\".csv\", index = False)\n",
    "    del encounter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ad604",
   "metadata": {},
   "source": [
    "### Ambulatory (AMB) Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb30efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW\n",
    "# Operationalization: We are interested in care settings: \n",
    "# Ambulatory (AMB)\n",
    "\n",
    "sub_blocks = [] \n",
    "# Read in all the truncated datasets and append to the master file\n",
    "for i in range(1, num_spreadsheets+1):\n",
    "    print(f'{i:04}')\n",
    "    # Use only \"patient_id\",\"encounter_id\",\"start_date\",\"end_date\",\"type\" columns\n",
    "    encounter = pd.read_csv(working_dir + \"encounter_NEW_\"+f'{i:04}'+\".csv\",\n",
    "                            parse_dates = [\"start_date\",\"end_date\"],\n",
    "                            dtype = {\"encounter_id\":str,\"patient_id\":str,\"type\":str})\n",
    "    # Exclude all \"start_date\" that occurs BEFORE 01/01/2022\n",
    "    encounter = encounter[encounter[\"start_date\"] >= pd.to_datetime(\"20220101\")]\n",
    "    # Include only \"Ambulatory\" (AMB) encounters\n",
    "    encounter = encounter[encounter[\"type\"].str.match(\"^AMB$\")]\n",
    "    # Fill missing \"end_date\" column values with 12/31/2022 as STRING\n",
    "    encounter[\"end_date\"] = encounter[\"end_date\"].fillna(\"20221231\").astype(str)\n",
    "    # Recast \"end_date\" columns as DATETIME\n",
    "    encounter[\"end_date\"] = pd.to_datetime(encounter[\"end_date\"])\n",
    "    # Reorder the columns as: \"patient_id\", \"encounter_id\", \"start_date\", \"end_date\", \"type\"\n",
    "    encounter = encounter[[\"patient_id\", \"encounter_id\", \"start_date\", \"end_date\", \"type\"]]\n",
    "    # Write rows to new dataframe - \"AMB_encounters\"\n",
    "    sub_blocks.append(encounter)\n",
    "    del encounter\n",
    "\n",
    "# Process the overall dataset\n",
    "AMB_encounters = pd.concat(sub_blocks, ignore_index = True)\n",
    "# Sort values by \"start_date\" (ascending order)\n",
    "AMB_encounters.sort_values(by = [\"start_date\",\"encounter_id\"], ascending = [True,False], inplace = True)\n",
    "# Drop duplicate encounters\n",
    "AMB_encounters.drop_duplicates(subset = [\"encounter_id\"], keep = \"first\", inplace = True)\n",
    "# Calculate the \"Length of Stay\" (LOS) variable\n",
    "AMB_encounters[\"LOS\"] = AMB_encounters[\"end_date\"] - AMB_encounters[\"start_date\"]\n",
    "# Adding \"1\" for values where \"start_date\" = \"end_date\"; Admitted and discharged on the same day\n",
    "AMB_encounters[\"LOS\"] = AMB_encounters[\"LOS\"].dt.days + 1\n",
    "# Some encounters have \"end_dates\" that come before their associated \"start_date\"; Removing these rows\n",
    "AMB_encounters = AMB_encounters[AMB_encounters[\"LOS\"] > 0]\n",
    "# Print the new dataframe shape\n",
    "print(\"AMB_encounters Shape:\", AMB_encounters.shape)\n",
    "# Write new \"AMB_encounters\" dataframes to CSV\n",
    "AMB_encounters.to_csv(working_dir + \"AMB_encounters.csv\", index = False)\n",
    "# Doublecheck to make sure the CSV was created correctly\n",
    "# AMB_encounters = pd.read_csv(database_dir + \"AMB_encounters.csv\", nrows = 1000)\n",
    "# display(AMB_encounters.head())\n",
    "del AMB_encounters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afedd38",
   "metadata": {},
   "source": [
    "### Emergency (EMER) Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c49458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW \n",
    "# Operationalization: We are interested in care settings: \n",
    "# Emergency (EMER)\n",
    "\n",
    "sub_blocks = [] \n",
    "for i in range(1, num_spreadsheets + 1):\n",
    "    print(f'{i:04}')\n",
    "    # Call in the encounter1.csv dataframe \n",
    "    # Use only \"patient_id\",\"encounter_id\",\"start_date\",\"end_date\",\"type\" columns\n",
    "    encounter = pd.read_csv(working_dir + \"encounter_NEW_\"+f'{i:04}'+\".csv\",\n",
    "                            parse_dates = [\"start_date\",\"end_date\"],\n",
    "                            dtype = {\"encounter_id\":str,\"patient_id\":str,\"type\":str})\n",
    "    # Exclude all \"start_date\" that occurs BEFORE 01/01/2022\n",
    "    encounter = encounter[encounter[\"start_date\"] >= pd.to_datetime(\"20220101\")]\n",
    "    # Include only \"Emergency\" (EMER) encounters\n",
    "    encounter = encounter[encounter[\"type\"].str.match(\"^EMER$\")]\n",
    "    # Fill missing \"end_date\" column values with 12/31/2022 as STRING\n",
    "    encounter[\"end_date\"] = encounter[\"end_date\"].fillna(\"20221231\").astype(str)\n",
    "    # Recast \"end_date\" columns as DATETIME\n",
    "    encounter[\"end_date\"] = pd.to_datetime(encounter[\"end_date\"])\n",
    "    # Reorder the columns as: \"patient_id\", \"encounter_id\", \"start_date\", \"end_date\", \"type\"\n",
    "    encounter = encounter[[\"patient_id\", \"encounter_id\", \"start_date\", \"end_date\", \"type\"]]\n",
    "    # Write rows to new dataframe - \"EMER_encounters\"\n",
    "    #EMER_encounters = pd.concat([EMER_encounters, encounter], ignore_index = True)\n",
    "    sub_blocks.append(encounter)\n",
    "    # Print the new dataframe shape\n",
    "    #print(EMER_encounters.shape)\n",
    "    # Delete encounter1 \n",
    "    del encounter\n",
    "\n",
    "EMER_encounters = pd.concat(sub_blocks, ignore_index = True)\n",
    "# Sort values by \"start_date\" (ascending order)\n",
    "EMER_encounters.sort_values(by = [\"start_date\",\"encounter_id\"], ascending = [True,False], inplace = True)\n",
    "# Drop duplicate encounters\n",
    "EMER_encounters.drop_duplicates(subset = [\"encounter_id\"], keep = \"first\", inplace = True)\n",
    "# Calculate the \"Length of Stay\" (LOS) variable\n",
    "EMER_encounters[\"LOS\"] = EMER_encounters[\"end_date\"] - EMER_encounters[\"start_date\"]\n",
    "# Adding \"1\" for values where \"start_date\" = \"end_date\"; Admitted and discharged on the same day\n",
    "EMER_encounters[\"LOS\"] = EMER_encounters[\"LOS\"].dt.days + 1\n",
    "# Some encounters have \"end_dates\" that come before their associated \"start_date\"; Removing these rows\n",
    "EMER_encounters = EMER_encounters[EMER_encounters[\"LOS\"] > 0]\n",
    "# Print the new dataframe shape\n",
    "print(\"EMER_encounters Shape:\", EMER_encounters.shape)\n",
    "# Write new \"EMER_encounters\" dataframes to CSV\n",
    "EMER_encounters.to_csv(working_dir + \"EMER_encounters.csv\", index = False)\n",
    "# Doublecheck to make sure the CSV was created correctly\n",
    "# EMER_encounters = pd.read_csv(database_dir + \"EMER_encounters.csv\", nrows = 1000)\n",
    "# display(EMER_encounters.head())\n",
    "del EMER_encounters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e6f46",
   "metadata": {},
   "source": [
    "### Inpatient (IMP) Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW \n",
    "# Operationalization: We are interested in care settings: \n",
    "# Inpatient Encounter (IMP)\n",
    "\n",
    "sub_blocks = []\n",
    "for i in range(1, num_spreadsheets+1):\n",
    "    print(f'{i:04}')\n",
    "    # Call in the encounter1.csv dataframe \n",
    "    # Use only \"patient_id\",\"encounter_id\",\"start_date\",\"end_date\",\"type\" columns\n",
    "    encounter = pd.read_csv(working_dir + \"encounter_NEW_\"+f'{i:04}'+\".csv\",\n",
    "                            parse_dates = [\"start_date\",\"end_date\"],\n",
    "                            dtype = {\"encounter_id\":str,\"patient_id\":str,\"type\":str})\n",
    "    # Exclude all \"start_date\" that occurs BEFORE 01/01/2022\n",
    "    encounter = encounter[encounter[\"start_date\"] >= pd.to_datetime(\"20220101\")]\n",
    "    # Include only \"Inpatient\" (IMP) encounters\n",
    "    encounter = encounter[encounter[\"type\"].str.match(\"^IMP$\")]\n",
    "    # Fill missing \"end_date\" column values with 12/31/2022 as STRING\n",
    "    encounter[\"end_date\"] = encounter[\"end_date\"].fillna(\"20221231\").astype(str)\n",
    "    # Recast \"end_date\" columns as DATETIME\n",
    "    encounter[\"end_date\"] = pd.to_datetime(encounter[\"end_date\"])\n",
    "    # Reorder the columns as: \"patient_id\", \"encounter_id\", \"start_date\", \"end_date\", \"type\"\n",
    "    encounter = encounter[[\"patient_id\", \"encounter_id\", \"start_date\", \"end_date\", \"type\"]]\n",
    "    # Write rows to new dataframe - \"IMP_encounters\"\n",
    "    # IMP_encounters = pd.concat([IMP_encounters, encounter], ignore_index = True)\n",
    "    sub_blocks.append(encounter)\n",
    "    # Print the new dataframe shape\n",
    "    #print(IMP_encounters.shape)\n",
    "    # Delete encounter1 \n",
    "    del encounter\n",
    "\n",
    "IMP_encounters = pd.concat(sub_blocks, ignore_index = True)\n",
    "# Sort values by \"start_date\" (ascending order)\n",
    "IMP_encounters.sort_values(by = [\"start_date\",\"encounter_id\"], ascending = [True,False], inplace = True)\n",
    "# Drop duplicate encounters\n",
    "IMP_encounters.drop_duplicates(subset = [\"encounter_id\"], keep = \"first\", inplace = True)\n",
    "# Calculate the \"Length of Stay\" (LOS) variable\n",
    "IMP_encounters[\"LOS\"] = IMP_encounters[\"end_date\"] - IMP_encounters[\"start_date\"]\n",
    "# Adding \"1\" for values where \"start_date\" = \"end_date\"; Admitted and discharged on the same day\n",
    "IMP_encounters[\"LOS\"] = IMP_encounters[\"LOS\"].dt.days + 1\n",
    "# Some encounters have \"end_dates\" that come before their associated \"start_date\"; Removing these rows\n",
    "IMP_encounters = IMP_encounters[IMP_encounters[\"LOS\"] > 0]\n",
    "# Print the new dataframe shape\n",
    "print(\"IMP_encounters Shape:\", IMP_encounters.shape)\n",
    "# Write new \"IMP_encounters\" dataframes to CSV\n",
    "IMP_encounters.to_csv(working_dir + \"INPAT_encounters.csv\", index = False)\n",
    "# Doublecheck to make sure the CSV was created correctly\n",
    "#IMP_encounters = pd.read_csv(database_dir + \"INPAT_encounters.csv\", nrows = 1000)\n",
    "display(IMP_encounters.head())\n",
    "del IMP_encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMP_encounters = pd.read_csv(working_dir + \"INPAT_encounters.csv\")\n",
    "print(\"Unique IMP Patients:\", len(IMP_encounters[\"patient_id\"].unique()))\n",
    "print(\"IMP_encounters:\",IMP_encounters.shape)\n",
    "del IMP_encounters\n",
    "\n",
    "AMB_encounters = pd.read_csv(working_dir + \"AMB_encounters.csv\")\n",
    "print(\"Unique AMB Patients\", len(AMB_encounters[\"patient_id\"].unique()))\n",
    "print(\"AMB_encounters:\",AMB_encounters.shape)\n",
    "del AMB_encounters\n",
    "\n",
    "EMER_encounters = pd.read_csv(working_dir + \"EMER_encounters.csv\")\n",
    "print(\"Unique EMER Patients:\",len(EMER_encounters[\"patient_id\"].unique()))\n",
    "print(\"EMER_encounters:\",EMER_encounters.shape)\n",
    "del EMER_encounters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f2b4e",
   "metadata": {},
   "source": [
    "### Clean Datasets Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efe449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the pattern to match the files\n",
    "# This pattern matches \"encounter_\" followed by exactly 4 digits and \".csv\"\n",
    "pattern = os.path.join(working_dir, \"encounter_NEW_????\"+\".csv\")\n",
    "\n",
    "# Use glob.glob to find all files that match the pattern\n",
    "matching_files = glob.glob(pattern)\n",
    "\n",
    "# Iterate over the list of matching files and delete each one\n",
    "for file_path in matching_files:\n",
    "    try:\n",
    "        #os.remove(file_path)\n",
    "        print(f\"Deleted file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting file {file_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
