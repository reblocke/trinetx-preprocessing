{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Data Check Documents for the Proper Data Control Insurance Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: output files will be placed in the working dir\n",
    "\n",
    "#PC: \n",
    "database_dir = r\"E:\\TriNetX\\\\\"   # Location where the database files are stored \n",
    "working_dir = r\"C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\\\\" #location where to read and right from (faster = better if space allows)\n",
    "\n",
    "#Mac \n",
    "#database_dir = r\"/Volumes/LOCKE STUDY/TriNetX\"   # Location where the database files are stored \n",
    "#working_dir = r\"/Users/blocke/TriNetX Working/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import logging\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import config\n",
    "import h5py\n",
    "\n",
    "#Create an output directory if it's not already there\n",
    "os.makedirs(os.path.join(working_dir[:-1], \"data_checks\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make HD5 Files with each type of data element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vital Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vital Signs\n",
    "start_time = time.time()\n",
    "store_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 853\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Vital Signs/vital_signs{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diagnoses \n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "num_spreadsheets = 1273\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Diagnosis/diagnosis{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2334\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"date\",\"value\",\"text_value\",\"units_of_measure\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Lab Results/lab_results{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedures\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 714\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_procedure_indicator\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Procedure/procedure{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2991\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"unique_id\",\"code_system\",\"code\",\"start_date\",\"route\",\"brand\",\"strength\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Medications/medication{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H5 Structure Check Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_structure(name, obj):\n",
    "    print(name)\n",
    "\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "lab_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "med_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "proc_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# Check structure in diag_unique_encounters.h5\n",
    "print(\"Structure of diag_unique_encounters.h5:\")\n",
    "with h5py.File(diag_path, 'r') as f:\n",
    "    f.visititems(print_structure)\n",
    "\n",
    "# Check structure in vitals_unique_encounters.h5\n",
    "print(\"Structure of vitals_unique_encounters.h5:\")\n",
    "with h5py.File(vitals_path, 'r') as f:\n",
    "    f.visititems(print_structure)\n",
    "\n",
    "# Check structure in lab_unique_encounters.h5\n",
    "print(\"Structure of lab_unique_encounters.h5:\")\n",
    "with h5py.File(lab_path, 'r') as f:\n",
    "    f.visititems(print_structure)\n",
    "\n",
    "# Check structure in med_unique_encounters.h5\n",
    "print(\"Structure of med_unique_encounters.h5:\")\n",
    "with h5py.File(med_path, 'r') as f:\n",
    "    f.visititems(print_structure)\n",
    "\n",
    "# Check structure in proc_unique_encounters.h5\n",
    "print(\"Structure of proc_unique_encounters.h5:\")\n",
    "with h5py.File(proc_path, 'r') as f:\n",
    "    f.visititems(print_structure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small enough to just use pandas\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_lab_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Pandas\n",
    "pdf = pd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pdf.to_csv(output_csv_path, index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vital Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vital Signs - small enough to do simply\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds \n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_proc_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds \n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Reading data from HDF5\n",
      "INFO:__main__:Dropping duplicates in the full dataset\n",
      "INFO:__main__:Computing the full dataset to remove duplicates\n"
     ]
    }
   ],
   "source": [
    "# Troubleshoot code block\n",
    "# Try this one first? \n",
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "logger.info(\"Reading data from HDF5\")\n",
    "# This is a hack to delay the compute\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Sample the data to check if it's a memory issue\n",
    "logger.info(\"Taking a sample of the data for testing\")\n",
    "sample_ddf = ddf.sample(frac=0.1).compute()\n",
    "logger.info(f\"Sample size: {sample_ddf.shape}\")\n",
    "\n",
    "# Check for duplicates in the sample\n",
    "logger.info(\"Dropping duplicates in the sample\")\n",
    "sample_ddf = sample_ddf.drop_duplicates()\n",
    "\n",
    "# Test writing the sample to CSV\n",
    "sample_output_path = os.path.join(working_dir[:-1], 'sample_clean_diag_unique_encounters.csv')\n",
    "logger.info(f\"Writing sample data to {sample_output_path}\")\n",
    "sample_ddf.to_csv(sample_output_path, index=False)\n",
    "\n",
    "# If the sample works, proceed with the full dataset\n",
    "logger.info(\"Dropping duplicates in the full dataset\")\n",
    "ddf = ddf.drop_duplicates()\n",
    "logger.info(\"Computing the full dataset to remove duplicates\")\n",
    "try:\n",
    "    ddf = ddf.compute()\n",
    "    logger.info(\"Full dataset computed successfully\")\n",
    "\n",
    "    # Write the full dataset to CSV\n",
    "    logger.info(f\"Writing full dataset to {output_csv_path}\")\n",
    "    ddf.to_csv(output_csv_path, index=False)\n",
    "except Exception as e:\n",
    "    logger.error(\"Error during compute or writing to CSV\", exc_info=True)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the working directory and HDF5 paths (update with your actual paths)\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '4.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=2,               # Number of worker processes (matching your 6 cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "logger.info(f\"Dask client created with dashboard at: {client.dashboard_link}\")\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "# Read the data using Dask\n",
    "logger.info(\"Reading data from HDF5\")\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Split the dataframe into 10 smaller chunks\n",
    "num_partitions = 10\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "logger.info(f\"Repartitioned dataframe into {num_partitions} partitions\")\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "logger.info(\"Dropping duplicates in each partition\")\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "logger.info(\"Computing the results for each partition\")\n",
    "computed_result = result.compute()\n",
    "\n",
    "# Combine results and drop duplicates again to ensure global uniqueness\n",
    "logger.info(\"Dropping duplicates from the combined result\")\n",
    "final_result = computed_result.drop_duplicates()\n",
    "\n",
    "# Write the final result to CSV\n",
    "logger.info(f\"Writing final dataset to {output_hdf_path}\")\n",
    "final_result.to_csv(output_hdf_path, index=False)\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "# Diagnoses\n",
    "\n",
    "# Configure Dask\n",
    "cluster = LocalCluster(\n",
    "    n_workers=6,              # Number of workers\n",
    "    threads_per_worker=1,     # Number of threads per worker\n",
    "    memory_limit='1.25GB',       # Memory limit per worker\n",
    "    processes=True,           # Use separate processes for each worker\n",
    "    dashboard_address=':8787' # Dashboard address http://localhost:8787\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "\n",
    "config.set({'distributed.worker.memory.target': 0.33,    # Spill to disk at 60% memory usage\n",
    "            'distributed.worker.memory.spill': 0.40,     # Spill to disk at 70% memory usage\n",
    "            'distributed.worker.memory.pause': 0.70,     # Pause worker at 80% memory usage\n",
    "            'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "            'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10          \n",
    "           })\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_hdf_path, index = False)\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Since compute is called, we now work with a Pandas DataFrame\n",
    "# Check and convert data type explicitly if needed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "if ddf['encounter_id'].dtype != 'object':\n",
    "    ddf['encounter_id'] = ddf['encounter_id'].astype('object')\n",
    "\n",
    "# Now, write the cleaned data frame back to HDF5\n",
    "with pd.HDFStore(output_hdf_path, 'w') as store:\n",
    "    store.put('unique_encounters', ddf, format='table', data_columns=True, index=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs - due to size, requires partitioning then recombination.\n",
    "# NOT NEEDED BECAUSE THE REGULAR ONE WORKS FINE\n",
    "\"\"\"\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_lab_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "#ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "#ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "\n",
    "num_partitions = 10\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "computed_result = result.compute()\n",
    "\n",
    "# Combine the partitions into a single Pandas DataFrame\n",
    "combined_df = pd.concat(computed_result)\n",
    "\n",
    "# Remove duplicates again to ensure global uniqueness - this is smaller. \n",
    "final_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file\n",
    "final_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "#hdf.to_csv(output_hdf_path, index = False)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Since compute is called, we now work with a Pandas DataFrame\n",
    "# Check and convert data type explicitly if needed\n",
    "if ddf['encounter_id'].dtype != 'object':\n",
    "    ddf['encounter_id'] = ddf['encounter_id'].astype('object')\n",
    "\n",
    "# Now, write the cleaned data frame back to HDF5\n",
    "with pd.HDFStore(output_hdf_path, 'w') as store:\n",
    "    store.put('unique_encounters', ddf, format='table', data_columns=True, index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import config\n",
    "\n",
    "# Configure Dask\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,              # Number of workers\n",
    "    threads_per_worker=1,     # Number of threads per worker\n",
    "    memory_limit='7.50GB',       # Memory limit per worker\n",
    "    processes=True,           # Use separate processes for each worker\n",
    "    dashboard_address=':8787' # Dashboard address http://localhost:8787\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "\n",
    "config.set({'distributed.worker.memory.target': 0.50,    # Spill to disk at 60% memory usage\n",
    "            'distributed.worker.memory.spill': 0.60,     # Spill to disk at 70% memory usage\n",
    "            'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "            'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "            'distributed.scheduler.allowed-failures': 4, # Set the allowed failures to 10          \n",
    "           })\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Repartition the dataframe into smaller chunks\n",
    "num_partitions = 2\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "computed_result = result.compute()\n",
    "#computed_result.to_csv(os.path.join(working_dir[:-1], 'dup_diag_unique_encounters.csv'), index=False)\n",
    "# Remove duplicates again to ensure global uniqueness\n",
    "computed_result = computed_result.drop_duplicates()\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file\n",
    "computed_result.to_csv(output_csv_path, single_file=True, index=False)\n",
    "\n",
    "print(f\"Combined CSV saved to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to clean up client\n",
    "\n",
    "# Start a Dask client with the dashboard\n",
    "print(client)\n",
    "\n",
    "# Open the dashboard URL printed in the output and monitor the tasks\n",
    "# Close the Dask client and cluster\n",
    "client.close()\n",
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Screens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambulatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Ambulatory\n",
    "# Read data from HDF5 files using Dask\n",
    "diag_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "print(diag_path)\n",
    "vitals_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "print(vitals_path)\n",
    "output_csv_path = os.path.join(working_dir[:-1], \"data_checks\", \"amb_enc_screen.csv\")\n",
    "print(output_csv_path)\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6pc 8mac cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "\n",
    "try:\n",
    "    # This head hack \n",
    "    diag_ddf = dd.read_csv(diag_path, usecols=['encounter_id']).head(100000000000, compute=False)\n",
    "    vitals_ddf = dd.read_csv(vitals_path, usecols=['encounter_id']).head(100000000000, compute=False)\n",
    "    print(\"Successfully read CSV files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV files into Dask DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    intersection_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "    print(\"Successfully merged DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    # intersection_ddf = intersection_ddf.persist()  \n",
    "    # Persist to avoid recomputation if I were to use this for multiple things - but currently don't\n",
    "    intersection_ddf.to_csv(output_csv_path, single_file=True, index=False)\n",
    "    print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inpatient and Emergency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Read data from HDF5 files using Dask\n",
    "diag_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "med_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv')\n",
    "proc_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv')\n",
    "lab_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv') \n",
    "\n",
    "output_csv_path = os.path.join(working_dir[:-1], \"data_checks\", \"inp_enc_screen.csv\")\n",
    "\n",
    "# Commented out if prior code block still running\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6pc 8mac cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "# Read the data using Dask and ensure 'encounter_id' is read as a string\n",
    "try:\n",
    "    # This head hack makes it so the reads are queued, rather than bringing everything in initially.\n",
    "    diag_ddf = dd.read_csv(diag_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    vitals_ddf = dd.read_csv(vitals_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    med_ddf = dd.read_csv(med_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    lab_ddf = dd.read_csv(lab_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    proc_ddf = dd.read_csv(proc_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    print(\"Successfully read CSV files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV files into Dask DataFrames: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    intersection_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "    \n",
    "    intersection_ddf = intersection_ddf.merge(med_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "\n",
    "    intersection_ddf = intersection_ddf.merge(proc_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "\n",
    "    intersection_ddf = intersection_ddf.merge(lab_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "    print(\"Successfully merged DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    # intersection_ddf = intersection_ddf.persist()  \n",
    "    # Persist to avoid recomputation if I were to use this for multiple things - but currently don't\n",
    "    intersection_ddf.to_csv(output_csv_path, single_file=True, index=False)\n",
    "    print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Scratchpad commands that didn't make the cut for various reseasons. \n",
    "\n",
    "Preserved in case need to pilfer them later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 Creation - deprecated version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEGACY? \n",
    "\n",
    "\"\"\"\n",
    "# Setup the storage file (HDF5)\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "\n",
    "num_spreadsheets = 1273\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"diagnosis_encounters.csv\")\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Diagnosis/diagnosis{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str}  # Ensure \"encounter_id\" is read as a string\n",
    "        )\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "# Read data using Dask\n",
    "dask_df = dd.read_hdf(store_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "dask_df = dask_df.drop_duplicates()\n",
    "\n",
    "# Compute and save to CSV\n",
    "dask_df.compute().to_csv(output_file, index=False)\n",
    "    \n",
    "print(f\"Unique encounter IDs with diagnoses reported are written to {output_file}\")\n",
    "\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version of Procedure Deduplications That Doesn't add Anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAYBE REMOVE:? Currently seems unsuccesful\n",
    "\"\"\"\n",
    "# Proc\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_proc_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '7.50GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6 cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Dask client created with dashboard at: {client.dashboard_link}\")\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.70,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.80,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute()\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "# Read the data using Dask\n",
    "try: \n",
    "    ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "    print(\"Successfully read H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 files into Dask DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    ddf = ddf.drop_duplicates().persist() # Remove duplicates\n",
    "    print(\"Successfully Dropped Duplicates\")\n",
    "except Exception as e:\n",
    "    print(f\"Error Dropping Duplicates: {e}\")\n",
    "\n",
    "\n",
    "try: \n",
    "    ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "    print(\"Successfully Compuated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error Computing: {e}\")\n",
    "\n",
    "try:\n",
    "    # Single-file replaces the compute stage and so it gets directly written to csv\n",
    "    ddf.to_csv(output_csv_path, single_file = True, index = False) \n",
    "    print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "    \n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versions of the Diagnosis De-duplication that didnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one doesn't work because the database is too big. \n",
    "\"\"\"start_time = time.time()\n",
    "\n",
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Pandas\n",
    "pdf = pd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pdf.to_csv(output_csv_path, index=False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to make the Inpatient and Emergency Screens break into chunks - no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Repartition the DataFrames into smaller chunks for better parallelism\n",
    "num_partitions = 16\n",
    "diag_ddf = diag_ddf.repartition(npartitions=num_partitions)\n",
    "vitals_ddf = vitals_ddf.repartition(npartitions=num_partitions)\n",
    "med_ddf = med_ddf.repartition(npartitions=num_partitions)\n",
    "lab_ddf = lab_ddf.repartition(npartitions=num_partitions)\n",
    "proc_ddf = proc_ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Ensure consistent partitioning and indexing\n",
    "def set_index_and_repartition(ddf):\n",
    "    ddf = ddf.set_index('encounter_id').repartition(npartitions=num_partitions)\n",
    "    return ddf\n",
    "\n",
    "diag_ddf = set_index_and_repartition(diag_ddf)\n",
    "vitals_ddf = set_index_and_repartition(vitals_ddf)\n",
    "med_ddf = set_index_and_repartition(med_ddf)\n",
    "lab_ddf = set_index_and_repartition(lab_ddf)\n",
    "proc_ddf = set_index_and_repartition(proc_ddf)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Merge DataFrames sequentially with intermediate persisting\n",
    "merged = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(med_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(proc_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(lab_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "# Drop duplicates just in case\n",
    "merged = merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file directly with Dask\n",
    "merged.to_csv(output_csv_path, single_file=True, index=False)\n",
    "\n",
    "print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
