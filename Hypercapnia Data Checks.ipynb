{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Data Check Documents for the Proper Data Control Insurance Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: output files will be placed in the working dir\n",
    "\n",
    "#PC: \n",
    "database_dir = r\"E:\\TriNetX\\\\\"   # Location where the database files are stored \n",
    "working_dir = r\"C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\\\\" #location where to read and right from (faster = better if space allows)\n",
    "\n",
    "#Mac \n",
    "#database_dir = r\"/Volumes/LOCKE STUDY/TriNetX\"   # Location where the database files are stored \n",
    "#working_dir = r\"/Users/blocke/TriNetX Working/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import logging\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import config\n",
    "import h5py\n",
    "\n",
    "#Create an output directory if it's not already there\n",
    "os.makedirs(os.path.join(working_dir[:-1], \"data_checks\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make HD5 Files with each type of data element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vital Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vital Signs\n",
    "start_time = time.time()\n",
    "store_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 853\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Vital Signs/vital_signs{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diagnoses \n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "num_spreadsheets = 1273\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Diagnosis/diagnosis{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2334\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"date\",\"value\",\"text_value\",\"units_of_measure\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Lab Results/lab_results{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedures\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 714\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_procedure_indicator\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Procedure/procedure{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2991\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"unique_id\",\"code_system\",\"code\",\"start_date\",\"route\",\"brand\",\"strength\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Medications/medication{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H5 Structure Check Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of diag_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 249836036 entries\n",
      "Structure of vitals_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 101603643 entries\n",
      "Structure of lab_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 100800073 entries\n",
      "Structure of med_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 159229970 entries\n",
      "Structure of proc_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 166304142 entries\n"
     ]
    }
   ],
   "source": [
    "def print_structure_and_count(name, obj):\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(f\"{name}: {len(obj)} entries\")\n",
    "    else:\n",
    "        print(name)\n",
    "\n",
    "def check_and_print_structure(file_path):\n",
    "    print(f\"Structure of {os.path.basename(file_path)}:\")\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        f.visititems(print_structure_and_count)\n",
    "\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "lab_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "med_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "proc_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# List of all HDF5 paths\n",
    "paths = [diag_path, vitals_path, lab_path, med_path, proc_path]\n",
    "\n",
    "for path in paths:\n",
    "    check_and_print_structure(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Attempt: try to merge the databases first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51287 instead\n",
      "  warnings.warn(\n",
      "2024-05-27 18:51:53,613 - distributed.protocol.core - CRITICAL - Failed to deserialize\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 128, in unpackb\n",
      "    ret = unpacker._unpack()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 565, in _unpack\n",
      "    ret.append(self._unpack(EX_CONSTRUCT))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 585, in _unpack\n",
      "    key = self._unpack(EX_CONSTRUCT)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 546, in _unpack\n",
      "    typ, n, obj = self._read_header()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 447, in _read_header\n",
      "    self._reserve(1)\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 420, in _reserve\n",
      "    raise OutOfData\n",
      "msgpack.exceptions.OutOfData\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\protocol\\core.py\", line 160, in loads\n",
      "    return msgpack.loads(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 130, in unpackb\n",
      "    raise ValueError(\"Unpack failed: incomplete input\")\n",
      "ValueError: Unpack failed: incomplete input\n",
      "2024-05-27 18:51:53,615 - distributed.core - ERROR - Exception while handling op register-client\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 128, in unpackb\n",
      "    ret = unpacker._unpack()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 565, in _unpack\n",
      "    ret.append(self._unpack(EX_CONSTRUCT))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 585, in _unpack\n",
      "    key = self._unpack(EX_CONSTRUCT)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 546, in _unpack\n",
      "    typ, n, obj = self._read_header()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 447, in _read_header\n",
      "    self._reserve(1)\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 420, in _reserve\n",
      "    raise OutOfData\n",
      "msgpack.exceptions.OutOfData\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\core.py\", line 968, in _handle_comm\n",
      "    result = await result\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\scheduler.py\", line 5532, in add_client\n",
      "    await self.handle_stream(comm=comm, extra={\"client\": client})\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\core.py\", line 1023, in handle_stream\n",
      "    msgs = await comm.read()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\comm\\tcp.py\", line 248, in read\n",
      "    msg = await from_frames(\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\comm\\utils.py\", line 78, in from_frames\n",
      "    res = _from_frames()\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\comm\\utils.py\", line 61, in _from_frames\n",
      "    return protocol.loads(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\protocol\\core.py\", line 160, in loads\n",
      "    return msgpack.loads(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 130, in unpackb\n",
      "    raise ValueError(\"Unpack failed: incomplete input\")\n",
      "ValueError: Unpack failed: incomplete input\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-858947' coro=<Server._handle_comm() done, defined at c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\core.py:874> exception=ValueError('Unpack failed: incomplete input')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 128, in unpackb\n",
      "    ret = unpacker._unpack()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 565, in _unpack\n",
      "    ret.append(self._unpack(EX_CONSTRUCT))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 585, in _unpack\n",
      "    key = self._unpack(EX_CONSTRUCT)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 546, in _unpack\n",
      "    typ, n, obj = self._read_header()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 447, in _read_header\n",
      "    self._reserve(1)\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 420, in _reserve\n",
      "    raise OutOfData\n",
      "msgpack.exceptions.OutOfData\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\core.py\", line 968, in _handle_comm\n",
      "    result = await result\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\scheduler.py\", line 5532, in add_client\n",
      "    await self.handle_stream(comm=comm, extra={\"client\": client})\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\core.py\", line 1023, in handle_stream\n",
      "    msgs = await comm.read()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\comm\\tcp.py\", line 248, in read\n",
      "    msg = await from_frames(\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\comm\\utils.py\", line 78, in from_frames\n",
      "    res = _from_frames()\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\comm\\utils.py\", line 61, in _from_frames\n",
      "    return protocol.loads(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\protocol\\core.py\", line 160, in loads\n",
      "    return msgpack.loads(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\msgpack\\fallback.py\", line 130, in unpackb\n",
      "    raise ValueError(\"Unpack failed: incomplete input\")\n",
      "ValueError: Unpack failed: incomplete input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read H5 files into Dask DataFrames.\n",
      "Error selecting columns: \"None of [Index(['unique_encounters'], dtype='object')] are in the [columns]\"\n",
      "Successfully renamed 'unique_encounters' to 'encounter_id'.\n",
      "Successfully converted 'encounter_id' to strings and trimmed whitespace.\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "('head-1-5-assign-8446a1ec8440ab60aa68d345a01e84cd', 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencounter_id\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28mprint\u001b[39m(diag_ddf\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Close the Dask client\u001b[39;00m\n\u001b[0;32m     65\u001b[0m client\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:1540\u001b[0m, in \u001b[0;36m_Frame.head\u001b[1;34m(self, n, npartitions, compute)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;66;03m# No need to warn if we're already looking at all partitions\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m safe \u001b[38;5;241m=\u001b[39m npartitions \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpartitions\n\u001b[1;32m-> 1540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_head(n\u001b[38;5;241m=\u001b[39mn, npartitions\u001b[38;5;241m=\u001b[39mnpartitions, compute\u001b[38;5;241m=\u001b[39mcompute, safe\u001b[38;5;241m=\u001b[39msafe)\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:1574\u001b[0m, in \u001b[0;36m_Frame._head\u001b[1;34m(self, n, npartitions, compute, safe)\u001b[0m\n\u001b[0;32m   1569\u001b[0m result \u001b[38;5;241m=\u001b[39m new_dd_object(\n\u001b[0;32m   1570\u001b[0m     graph, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdivisions[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdivisions[npartitions]]\n\u001b[0;32m   1571\u001b[0m )\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[1;32m-> 1574\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask\\base.py:342\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask\\base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\client.py:2245\u001b[0m, in \u001b[0;36mClient._gather\u001b[1;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[0;32m   2243\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   2246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2247\u001b[0m     bad_keys\u001b[38;5;241m.\u001b[39madd(key)\n",
      "\u001b[1;31mCancelledError\u001b[0m: ('head-1-5-assign-8446a1ec8440ab60aa68d345a01e84cd', 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "\n",
    "# Define paths\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "output_path = os.path.join(working_dir[:-1], 'merged_unique_encounters.csv')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6GB'\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,               # Number of worker processes\n",
    "    threads_per_worker=1,      # Number of threads per worker\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.70,\n",
    "    'distributed.worker.memory.spill': 0.80,\n",
    "    'distributed.worker.memory.pause': 0.80,\n",
    "    'distributed.worker.memory.terminate': 0.95,\n",
    "    'distributed.scheduler.allowed-failures': 10,\n",
    "})\n",
    "\n",
    "# Read the data using Dask\n",
    "try:\n",
    "    diag_ddf = dd.read_hdf(diag_path, 'unique_encounters/table')\n",
    "    vitals_ddf = dd.read_hdf(vitals_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    diag_ddf = diag_ddf[['unique_encounters']]\n",
    "    vitals_ddf = vitals_ddf[['unique_encounters']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting columns: {e}\")\n",
    "\n",
    "# Rename the 'unique_encounters' column to 'encounter_id'\n",
    "try:\n",
    "    diag_ddf = diag_ddf.rename(columns={'unique_encounters': 'encounter_id'})\n",
    "    vitals_ddf = vitals_ddf.rename(columns={'unique_encounters': 'encounter_id'})\n",
    "    print(\"Successfully renamed 'unique_encounters' to 'encounter_id'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error renaming columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    diag_ddf['encounter_id'] = diag_ddf['encounter_id'].astype(str).str.strip()\n",
    "    vitals_ddf['encounter_id'] = vitals_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing 'encounter_id': {e}\")\n",
    "\n",
    "print(diag_ddf.head())\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Delayed('int-c8a2ddc3-c486-43e0-947c-c9a83f6b097c'), 1)\n",
      "  encounter_id\n",
      "0          GRB\n",
      "1           Gh\n",
      "2          GBB\n",
      "3          GhB\n",
      "4           GR\n",
      "(Delayed('int-6d6da18e-4f2f-4288-a9d4-526ec25da1fd'), 1)\n",
      "    encounter_id\n",
      "0             GR\n",
      "1            GhB\n",
      "17            Gh\n",
      "21           GBB\n",
      "183          GhN\n",
      "Error during compute or writing to CSV 'unique_encounters'\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "output_path = os.path.join(working_dir[:-1], 'merged_unique_encounters.csv')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6GB'\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,               # Number of worker processes\n",
    "    threads_per_worker=1,      # Number of threads per worker\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.70,\n",
    "    'distributed.worker.memory.spill': 0.80,\n",
    "    'distributed.worker.memory.pause': 0.80,\n",
    "    'distributed.worker.memory.terminate': 0.95,\n",
    "    'distributed.scheduler.allowed-failures': 10,\n",
    "})\n",
    "\n",
    "try: \n",
    "    # Read the data using Dask\n",
    "    diag_ddf = dd.read_hdf(diag_path, 'unique_encounters')\n",
    "    vitals_ddf = dd.read_hdf(vitals_path, 'unique_encounters')\n",
    "\n",
    "    # Ensure the columns are named consistently if not already\n",
    "    diag_ddf = diag_ddf.rename(columns={diag_ddf.columns[0]: 'encounter_id'})\n",
    "    vitals_ddf = vitals_ddf.rename(columns={vitals_ddf.columns[0]: 'encounter_id'})\n",
    "    print(diag_ddf.shape)\n",
    "    print(diag_ddf.head())\n",
    "    print(vitals_ddf.shape)\n",
    "    print(vitals_ddf.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error during reading {e}\")\n",
    "\n",
    "try:\n",
    "    # Perform the merge operation to keep only common \"unique_encounters\"\n",
    "    merged_ddf = diag_ddf.merge(vitals_ddf, on='unique_encounters', how='inner')\n",
    "\n",
    "    # Persist the intermediate result to avoid recomputation\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "\n",
    "    # Write the result to a single CSV file\n",
    "    merged_ddf.to_csv(output_path, single_file=True, index=False)\n",
    "\n",
    "    print(f\"Merged unique encounters saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during compute or writing to CSV {e}\")\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small enough to just use pandas\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_lab_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Pandas\n",
    "pdf = pd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pdf.to_csv(output_csv_path, index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vital Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vital Signs - small enough to do simply\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds \n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_proc_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds \n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Reading data from HDF5\n",
      "INFO:__main__:Dropping duplicates in the full dataset\n",
      "INFO:__main__:Computing the full dataset to remove duplicates\n"
     ]
    }
   ],
   "source": [
    "# Troubleshoot code block\n",
    "# Try this one first? \n",
    "#  WHICH DIAGNOSIS VERSION WORKS? - Allegedly this one does on mac. \n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "logger.info(\"Reading data from HDF5\")\n",
    "# This is a hack to delay the compute\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Sample the data to check if it's a memory issue\n",
    "logger.info(\"Taking a sample of the data for testing\")\n",
    "sample_ddf = ddf.sample(frac=0.1).compute()\n",
    "logger.info(f\"Sample size: {sample_ddf.shape}\")\n",
    "\n",
    "# Check for duplicates in the sample\n",
    "logger.info(\"Dropping duplicates in the sample\")\n",
    "sample_ddf = sample_ddf.drop_duplicates()\n",
    "\n",
    "# Test writing the sample to CSV\n",
    "sample_output_path = os.path.join(working_dir[:-1], 'sample_clean_diag_unique_encounters.csv')\n",
    "logger.info(f\"Writing sample data to {sample_output_path}\")\n",
    "sample_ddf.to_csv(sample_output_path, index=False)\n",
    "\n",
    "# If the sample works, proceed with the full dataset\n",
    "logger.info(\"Dropping duplicates in the full dataset\")\n",
    "ddf = ddf.drop_duplicates()\n",
    "logger.info(\"Computing the full dataset to remove duplicates\")\n",
    "try:\n",
    "    ddf = ddf.compute()\n",
    "    logger.info(\"Full dataset computed successfully\")\n",
    "\n",
    "    # Write the full dataset to CSV\n",
    "    logger.info(f\"Writing full dataset to {output_csv_path}\")\n",
    "    ddf.to_csv(output_csv_path, index=False)\n",
    "except Exception as e:\n",
    "    logger.error(\"Error during compute or writing to CSV\", exc_info=True)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Screens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambulatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\clean_diag_unique_encounters.csv\n",
      "C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\clean_vitals_unique_encounters.csv\n",
      "C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\data_checks\\amb_enc_screen.csv\n",
      "Successfully read CSV files into Dask DataFrames.\n",
      "Successfully merged DataFrames.\n",
      "Intersection CSV saved to: C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\data_checks\\amb_enc_screen.csv\n",
      "Shape of output: (2659269, 1)\n",
      "Executed in 0 hours, 0 minutes, and 15.94 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5311"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Ambulatory\n",
    "# Read data from HDF5 files using Dask\n",
    "diag_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "print(diag_path)\n",
    "vitals_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "print(vitals_path)\n",
    "output_csv_path = os.path.join(working_dir[:-1], \"data_checks\", \"amb_enc_screen.csv\")\n",
    "print(output_csv_path)\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6pc 8mac cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "\n",
    "try:\n",
    "    # This head hack \n",
    "    diag_ddf = dd.read_csv(diag_path, usecols=['encounter_id']).head(100000000000, compute=False)\n",
    "    vitals_ddf = dd.read_csv(vitals_path, usecols=['encounter_id']).head(100000000000, compute=False)\n",
    "    print(\"Successfully read CSV files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV files into Dask DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    intersection_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "    print(\"Successfully merged DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    # intersection_ddf = intersection_ddf.persist()  \n",
    "    # Persist to avoid recomputation if I were to use this for multiple things - but currently don't\n",
    "    intersection_ddf.to_csv(output_csv_path, single_file=True, index=False)\n",
    "    print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inpatient and Emergency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read CSV files into Dask DataFrames.\n",
      "Diag DF shape (Delayed('int-bd0dec00-131d-45e6-ad81-972d72490991'), 1)\n",
      "post vitals DF shape (Delayed('int-0814e485-58bb-4c5a-8052-4629b8c0bdea'), 1)\n",
      "post encounters DF shape (Delayed('int-e078815b-e4fc-42ab-8a55-aee486778549'), 1)\n",
      "post procDF shape (Delayed('int-620a6dac-38e1-4fb6-8994-e1e74f01c0e2'), 1)\n",
      "post lab DF shape (Delayed('int-3a8bd936-bee6-4ffd-90a1-db8e8f995946'), 1)\n",
      "Successfully merged DataFrames.\n",
      "Intersection CSV saved to: C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\data_checks\\inp_enc_screen.csv\n",
      "Shape of output: (380772, 1)\n",
      "Executed in 0 hours, 0 minutes, and 31.27 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3119"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Read data from HDF5 files using Dask\n",
    "diag_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "med_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv')\n",
    "proc_path = os.path.join(working_dir[:-1], 'clean_proc_unique_encounters.csv')\n",
    "lab_path = os.path.join(working_dir[:-1], 'clean_lab_unique_encounters.csv') \n",
    "\n",
    "output_csv_path = os.path.join(working_dir[:-1], \"data_checks\", \"inp_enc_screen.csv\")\n",
    "\n",
    "# Commented out if prior code block still running\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6pc 8mac cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "# Read the data using Dask and ensure 'encounter_id' is read as a string\n",
    "try:\n",
    "    # This head hack makes it so the reads are queued, rather than bringing everything in initially.\n",
    "    diag_ddf = dd.read_csv(diag_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    vitals_ddf = dd.read_csv(vitals_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    med_ddf = dd.read_csv(med_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    lab_ddf = dd.read_csv(lab_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    proc_ddf = dd.read_csv(proc_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    print(\"Successfully read CSV files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV files into Dask DataFrames: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    ddf_shape = diag_ddf.shape\n",
    "    print(f\"Diag DF shape {ddf_shape}\")\n",
    "\n",
    "    intersection_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "    ddf_shape = intersection_ddf.shape\n",
    "    print(f\"post vitals DF shape {ddf_shape}\")\n",
    "\n",
    "    intersection_ddf = intersection_ddf.merge(med_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "    ddf_shape = intersection_ddf.shape\n",
    "    print(f\"post encounters DF shape {ddf_shape}\")\n",
    "\n",
    "    intersection_ddf = intersection_ddf.merge(proc_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "    ddf_shape = intersection_ddf.shape\n",
    "    print(f\"post procDF shape {ddf_shape}\")\n",
    "\n",
    "    intersection_ddf = intersection_ddf.merge(lab_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "    ddf_shape = intersection_ddf.shape\n",
    "    print(f\"post lab DF shape {ddf_shape}\")\n",
    "    print(\"Successfully merged DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    # intersection_ddf = intersection_ddf.persist()  \n",
    "    # Persist to avoid recomputation if I were to use this for multiple things - but currently don't\n",
    "    intersection_ddf.to_csv(output_csv_path, single_file=True, index=False)\n",
    "    print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Scratchpad commands that didn't make the cut for various reseasons. \n",
    "\n",
    "Preserved in case need to pilfer them later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 Creation - deprecated version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEGACY? \n",
    "\n",
    "\"\"\"\n",
    "# Setup the storage file (HDF5)\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "\n",
    "num_spreadsheets = 1273\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"diagnosis_encounters.csv\")\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Diagnosis/diagnosis{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str}  # Ensure \"encounter_id\" is read as a string\n",
    "        )\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "# Read data using Dask\n",
    "dask_df = dd.read_hdf(store_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "dask_df = dask_df.drop_duplicates()\n",
    "\n",
    "# Compute and save to CSV\n",
    "dask_df.compute().to_csv(output_file, index=False)\n",
    "    \n",
    "print(f\"Unique encounter IDs with diagnoses reported are written to {output_file}\")\n",
    "\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version of Procedure Deduplications That Doesn't add Anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAYBE REMOVE:? Currently seems unsuccesful\n",
    "\"\"\"\n",
    "# Proc\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_proc_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '7.50GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6 cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Dask client created with dashboard at: {client.dashboard_link}\")\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.70,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.80,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute()\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "# Read the data using Dask\n",
    "try: \n",
    "    ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "    print(\"Successfully read H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 files into Dask DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    ddf = ddf.drop_duplicates().persist() # Remove duplicates\n",
    "    print(\"Successfully Dropped Duplicates\")\n",
    "except Exception as e:\n",
    "    print(f\"Error Dropping Duplicates: {e}\")\n",
    "\n",
    "\n",
    "try: \n",
    "    ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "    print(\"Successfully Compuated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error Computing: {e}\")\n",
    "\n",
    "try:\n",
    "    # Single-file replaces the compute stage and so it gets directly written to csv\n",
    "    ddf.to_csv(output_csv_path, single_file = True, index = False) \n",
    "    print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "    \n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs - due to size, requires partitioning then recombination.\n",
    "# NOT NEEDED BECAUSE THE REGULAR ONE WORKS FINE\n",
    "\"\"\"\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_lab_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "#ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "#ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "\n",
    "num_partitions = 10\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "computed_result = result.compute()\n",
    "\n",
    "# Combine the partitions into a single Pandas DataFrame\n",
    "combined_df = pd.concat(computed_result)\n",
    "\n",
    "# Remove duplicates again to ensure global uniqueness - this is smaller. \n",
    "final_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file\n",
    "final_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "#hdf.to_csv(output_hdf_path, index = False)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Since compute is called, we now work with a Pandas DataFrame\n",
    "# Check and convert data type explicitly if needed\n",
    "if ddf['encounter_id'].dtype != 'object':\n",
    "    ddf['encounter_id'] = ddf['encounter_id'].astype('object')\n",
    "\n",
    "# Now, write the cleaned data frame back to HDF5\n",
    "with pd.HDFStore(output_hdf_path, 'w') as store:\n",
    "    store.put('unique_encounters', ddf, format='table', data_columns=True, index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versions of the Diagnosis De-duplication that didnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one doesn't work because the database is too big. \n",
    "\"\"\"start_time = time.time()\n",
    "\n",
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Pandas\n",
    "pdf = pd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pdf.to_csv(output_csv_path, index=False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the working directory and HDF5 paths (update with your actual paths)\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '4.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=2,               # Number of worker processes (matching your 6 cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "logger.info(f\"Dask client created with dashboard at: {client.dashboard_link}\")\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "# Read the data using Dask\n",
    "logger.info(\"Reading data from HDF5\")\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Split the dataframe into 10 smaller chunks\n",
    "num_partitions = 10\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "logger.info(f\"Repartitioned dataframe into {num_partitions} partitions\")\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "logger.info(\"Dropping duplicates in each partition\")\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "logger.info(\"Computing the results for each partition\")\n",
    "computed_result = result.compute()\n",
    "\n",
    "# Combine results and drop duplicates again to ensure global uniqueness\n",
    "logger.info(\"Dropping duplicates from the combined result\")\n",
    "final_result = computed_result.drop_duplicates()\n",
    "\n",
    "# Write the final result to CSV\n",
    "logger.info(f\"Writing final dataset to {output_hdf_path}\")\n",
    "final_result.to_csv(output_hdf_path, index=False)\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "# Diagnoses\n",
    "\n",
    "# Configure Dask\n",
    "cluster = LocalCluster(\n",
    "    n_workers=6,              # Number of workers\n",
    "    threads_per_worker=1,     # Number of threads per worker\n",
    "    memory_limit='1.25GB',       # Memory limit per worker\n",
    "    processes=True,           # Use separate processes for each worker\n",
    "    dashboard_address=':8787' # Dashboard address http://localhost:8787\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "\n",
    "config.set({'distributed.worker.memory.target': 0.33,    # Spill to disk at 60% memory usage\n",
    "            'distributed.worker.memory.spill': 0.40,     # Spill to disk at 70% memory usage\n",
    "            'distributed.worker.memory.pause': 0.70,     # Pause worker at 80% memory usage\n",
    "            'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "            'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10          \n",
    "           })\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_hdf_path, index = False)\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Since compute is called, we now work with a Pandas DataFrame\n",
    "# Check and convert data type explicitly if needed\n",
    "\n",
    "\n",
    "if ddf['encounter_id'].dtype != 'object':\n",
    "    ddf['encounter_id'] = ddf['encounter_id'].astype('object')\n",
    "\n",
    "# Now, write the cleaned data frame back to HDF5\n",
    "with pd.HDFStore(output_hdf_path, 'w') as store:\n",
    "    store.put('unique_encounters', ddf, format='table', data_columns=True, index=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import config\n",
    "\n",
    "# Configure Dask\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,              # Number of workers\n",
    "    threads_per_worker=1,     # Number of threads per worker\n",
    "    memory_limit='7.50GB',       # Memory limit per worker\n",
    "    processes=True,           # Use separate processes for each worker\n",
    "    dashboard_address=':8787' # Dashboard address http://localhost:8787\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "\n",
    "config.set({'distributed.worker.memory.target': 0.50,    # Spill to disk at 60% memory usage\n",
    "            'distributed.worker.memory.spill': 0.60,     # Spill to disk at 70% memory usage\n",
    "            'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "            'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "            'distributed.scheduler.allowed-failures': 4, # Set the allowed failures to 10          \n",
    "           })\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Repartition the dataframe into smaller chunks\n",
    "num_partitions = 2\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "computed_result = result.compute()\n",
    "#computed_result.to_csv(os.path.join(working_dir[:-1], 'dup_diag_unique_encounters.csv'), index=False)\n",
    "# Remove duplicates again to ensure global uniqueness\n",
    "computed_result = computed_result.drop_duplicates()\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file\n",
    "computed_result.to_csv(output_csv_path, single_file=True, index=False)\n",
    "\n",
    "print(f\"Combined CSV saved to: {output_csv_path}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to make the Inpatient and Emergency Screens break into chunks - no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Repartition the DataFrames into smaller chunks for better parallelism\n",
    "num_partitions = 16\n",
    "diag_ddf = diag_ddf.repartition(npartitions=num_partitions)\n",
    "vitals_ddf = vitals_ddf.repartition(npartitions=num_partitions)\n",
    "med_ddf = med_ddf.repartition(npartitions=num_partitions)\n",
    "lab_ddf = lab_ddf.repartition(npartitions=num_partitions)\n",
    "proc_ddf = proc_ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Ensure consistent partitioning and indexing\n",
    "def set_index_and_repartition(ddf):\n",
    "    ddf = ddf.set_index('encounter_id').repartition(npartitions=num_partitions)\n",
    "    return ddf\n",
    "\n",
    "diag_ddf = set_index_and_repartition(diag_ddf)\n",
    "vitals_ddf = set_index_and_repartition(vitals_ddf)\n",
    "med_ddf = set_index_and_repartition(med_ddf)\n",
    "lab_ddf = set_index_and_repartition(lab_ddf)\n",
    "proc_ddf = set_index_and_repartition(proc_ddf)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Merge DataFrames sequentially with intermediate persisting\n",
    "merged = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(med_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(proc_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(lab_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "# Drop duplicates just in case\n",
    "merged = merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file directly with Dask\n",
    "merged.to_csv(output_csv_path, single_file=True, index=False)\n",
    "\n",
    "print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to clean up client\n",
    "\"\"\"\n",
    "# Start a Dask client with the dashboard\n",
    "print(client)\n",
    "\n",
    "# Open the dashboard URL printed in the output and monitor the tasks\n",
    "# Close the Dask client and cluster\n",
    "client.close()\n",
    "cluster.close()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
