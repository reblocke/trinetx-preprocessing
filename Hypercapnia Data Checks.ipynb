{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Data Check Documents for the Proper Data Control Insurance Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: output files will be placed in the working dir\n",
    "\n",
    "#PC: \n",
    "#database_dir = r\"E:\\TriNetX\\\\\"   # Location where the database files are stored \n",
    "#working_dir = r\"C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\\\\" #location where to read and right from (faster = better if space allows)\n",
    "\n",
    "#Mac \n",
    "database_dir = r\"/Volumes/LOCKE STUDY/TriNetX\"   # Location where the database files are stored \n",
    "working_dir = r\"/Users/blocke/TriNetX Working/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import logging\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "#Create an output directory if it's not already there\n",
    "os.makedirs(os.path.join(working_dir[:-1], \"data_checks\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make HD5 Files with each type of data element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vital Signs\n",
    "start_time = time.time()\n",
    "store_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 853\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Vital Signs/vital_signs{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diagnoses \n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "num_spreadsheets = 1273\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Diagnosis/diagnosis{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2334\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"date\",\"value\",\"text_value\",\"units_of_measure\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Lab Results/lab_results{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedures\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 714\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_procedure_indicator\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Procedure/procedure{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2991\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"unique_id\",\"code_system\",\"code\",\"start_date\",\"route\",\"brand\",\"strength\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Medications/medication{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small enough to just use pandas\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_lab_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Pandas\n",
    "pdf = pd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pdf.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem - not small enough\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Pandas\n",
    "pdf = pd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pdf.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vital Signs\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_hdf_path, index = False)\n",
    "\"\"\"\n",
    "# Since compute is called, we now work with a Pandas DataFrame\n",
    "# Check and convert data type explicitly if needed\n",
    "if ddf['encounter_id'].dtype != 'object':\n",
    "    ddf['encounter_id'] = ddf['encounter_id'].astype('object')\n",
    "\n",
    "# Now, write the cleaned data frame back to HDF5\n",
    "with pd.HDFStore(output_hdf_path, 'w') as store:\n",
    "    store.put('unique_encounters', ddf, format='table', data_columns=True, index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dask client created with dashboard at: http://127.0.0.1:8787/status\n",
      "INFO:__main__:Reading data from HDF5\n",
      "INFO:__main__:Repartitioned dataframe into 10 partitions\n",
      "INFO:__main__:Dropping duplicates in each partition\n",
      "INFO:__main__:Computing the results for each partition\n",
      "2024-05-18 09:27:10,696 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56207 (pid=12912) exceeded 95% memory budget. Restarting...\n",
      "2024-05-18 09:27:10,794 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56207 (pid=12912) is slow to terminate; trying again\n",
      "2024-05-18 09:27:10,881 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56207 (pid=12912) is slow to terminate; trying again\n",
      "2024-05-18 09:27:11,188 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-18 09:27:37,798 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56210 (pid=18260) exceeded 95% memory budget. Restarting...\n",
      "2024-05-18 09:27:38,254 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-18 09:28:49,304 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56258 (pid=14480) exceeded 95% memory budget. Restarting...\n",
      "2024-05-18 09:28:49,399 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56258 (pid=14480) is slow to terminate; trying again\n",
      "2024-05-18 09:28:49,823 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-18 09:30:37,795 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56245 (pid=18168) exceeded 95% memory budget. Restarting...\n",
      "2024-05-18 09:30:37,877 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56245 (pid=18168) is slow to terminate; trying again\n",
      "2024-05-18 09:30:38,272 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-18 09:31:02,579 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56284 (pid=14424) exceeded 95% memory budget. Restarting...\n",
      "2024-05-18 09:31:02,856 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-18 09:31:59,703 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56323 (pid=7620) exceeded 95% memory budget. Restarting...\n",
      "2024-05-18 09:31:59,812 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56323 (pid=7620) is slow to terminate; trying again\n",
      "2024-05-18 09:32:00,147 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-18 09:32:24,083 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:56312 (pid=1604) exceeded 95% memory budget. Restarting...\n",
      "2024-05-18 09:32:24,361 - distributed.nanny - WARNING - Restarting worker\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-464' coro=<Client._gather.<locals>.wait() done, defined at c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\client.py:2209> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\client.py\", line 2218, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Compute the result\u001b[39;00m\n\u001b[0;32m     50\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing the results for each partition\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m computed_result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Combine results and drop duplicates again to ensure global uniqueness\u001b[39;00m\n\u001b[0;32m     54\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropping duplicates from the combined result\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask\\base.py:342\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask\\base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the working directory and HDF5 paths (update with your actual paths)\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '4.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=2,               # Number of worker processes (matching your 6 cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "logger.info(f\"Dask client created with dashboard at: {client.dashboard_link}\")\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "# Read the data using Dask\n",
    "logger.info(\"Reading data from HDF5\")\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Split the dataframe into 4 smaller chunks\n",
    "num_partitions = 10\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "logger.info(f\"Repartitioned dataframe into {num_partitions} partitions\")\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "logger.info(\"Dropping duplicates in each partition\")\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "logger.info(\"Computing the results for each partition\")\n",
    "computed_result = result.compute()\n",
    "\n",
    "# Combine results and drop duplicates again to ensure global uniqueness\n",
    "logger.info(\"Dropping duplicates from the combined result\")\n",
    "final_result = computed_result.drop_duplicates()\n",
    "\n",
    "# Write the final result to CSV\n",
    "logger.info(f\"Writing final dataset to {output_hdf_path}\")\n",
    "final_result.to_csv(output_hdf_path, index=False)\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnoses\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import config\n",
    "\n",
    "# Configure Dask\n",
    "cluster = LocalCluster(\n",
    "    n_workers=6,              # Number of workers\n",
    "    threads_per_worker=1,     # Number of threads per worker\n",
    "    memory_limit='1.25GB',       # Memory limit per worker\n",
    "    processes=True,           # Use separate processes for each worker\n",
    "    dashboard_address=':8787' # Dashboard address http://localhost:8787\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "\n",
    "config.set({'distributed.worker.memory.target': 0.33,    # Spill to disk at 60% memory usage\n",
    "            'distributed.worker.memory.spill': 0.40,     # Spill to disk at 70% memory usage\n",
    "            'distributed.worker.memory.pause': 0.70,     # Pause worker at 80% memory usage\n",
    "            'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "            'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10          \n",
    "           })\n",
    "\n",
    "\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_hdf_path, index = False)\n",
    "\n",
    "\n",
    "# Since compute is called, we now work with a Pandas DataFrame\n",
    "# Check and convert data type explicitly if needed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "if ddf['encounter_id'].dtype != 'object':\n",
    "    ddf['encounter_id'] = ddf['encounter_id'].astype('object')\n",
    "\n",
    "# Now, write the cleaned data frame back to HDF5\n",
    "with pd.HDFStore(output_hdf_path, 'w') as store:\n",
    "    store.put('unique_encounters', ddf, format='table', data_columns=True, index=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to clean up client\n",
    "\n",
    "# Start a Dask client with the dashboard\n",
    "print(client)\n",
    "\n",
    "# Open the dashboard URL printed in the output and monitor the tasks\n",
    "# Close the Dask client and cluster\n",
    "client.close()\n",
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshoot code block\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "logger.info(\"Reading data from HDF5\")\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Sample the data to check if it's a memory issue\n",
    "logger.info(\"Taking a sample of the data for testing\")\n",
    "sample_ddf = ddf.sample(frac=0.1).compute()\n",
    "logger.info(f\"Sample size: {sample_ddf.shape}\")\n",
    "\n",
    "# Check for duplicates in the sample\n",
    "logger.info(\"Dropping duplicates in the sample\")\n",
    "sample_ddf = sample_ddf.drop_duplicates()\n",
    "\n",
    "# Test writing the sample to CSV\n",
    "sample_output_path = os.path.join(working_dir[:-1], 'sample_clean_diag_unique_encounters.csv')\n",
    "logger.info(f\"Writing sample data to {sample_output_path}\")\n",
    "sample_ddf.to_csv(sample_output_path, index=False)\n",
    "\n",
    "# If the sample works, proceed with the full dataset\n",
    "logger.info(\"Dropping duplicates in the full dataset\")\n",
    "ddf = ddf.drop_duplicates()\n",
    "logger.info(\"Computing the full dataset to remove duplicates\")\n",
    "try:\n",
    "    ddf = ddf.compute()\n",
    "    logger.info(\"Full dataset computed successfully\")\n",
    "\n",
    "    # Write the full dataset to CSV\n",
    "    logger.info(f\"Writing full dataset to {output_hdf_path}\")\n",
    "    ddf.to_csv(output_hdf_path, index=False)\n",
    "except Exception as e:\n",
    "    logger.error(\"Error during compute or writing to CSV\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m computed_result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Combine the partitions into a single Pandas DataFrame\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(computed_result)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Remove duplicates again to ensure global uniqueness - this is smaller. \u001b[39;00m\n\u001b[0;32m     29\u001b[0m final_df \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:380\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    381\u001b[0m     objs,\n\u001b[0;32m    382\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    383\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[0;32m    384\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[0;32m    385\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[0;32m    386\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[0;32m    387\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m    388\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[0;32m    389\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    390\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    391\u001b[0m )\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:417\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    405\u001b[0m     objs: Iterable[Series \u001b[38;5;241m|\u001b[39m DataFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, Series \u001b[38;5;241m|\u001b[39m DataFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    415\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(objs, (ABCSeries, ABCDataFrame, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[1;32m--> 417\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    418\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be an iterable of pandas \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjects, you passed an object of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(objs)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    420\u001b[0m         )\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m join \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintersect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\""
     ]
    }
   ],
   "source": [
    "# Labs - due to size, requires partitioning then recombination.\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_lab_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "#ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "#ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "\n",
    "num_partitions = 10\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "computed_result = result.compute()\n",
    "\n",
    "# Combine the partitions into a single Pandas DataFrame\n",
    "combined_df = pd.concat(computed_result)\n",
    "\n",
    "# Remove duplicates again to ensure global uniqueness - this is smaller. \n",
    "final_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file\n",
    "final_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "#hdf.to_csv(output_hdf_path, index = False)\n",
    "\"\"\"\n",
    "# Since compute is called, we now work with a Pandas DataFrame\n",
    "# Check and convert data type explicitly if needed\n",
    "if ddf['encounter_id'].dtype != 'object':\n",
    "    ddf['encounter_id'] = ddf['encounter_id'].astype('object')\n",
    "\n",
    "# Now, write the cleaned data frame back to HDF5\n",
    "with pd.HDFStore(output_hdf_path, 'w') as store:\n",
    "    store.put('unique_encounters', ddf, format='table', data_columns=True, index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 12:21:14,760 - distributed.scheduler - WARNING - Worker failed to heartbeat within 300 seconds. Closing: <WorkerState 'tcp://127.0.0.1:50071', name: 0, status: running, memory: 3, processing: 2>\n",
      "2024-05-18 12:21:33,505 - distributed.nanny - WARNING - Worker process still alive after 3.1862852478027346 seconds, killing\n",
      "2024-05-18 12:54:56,474 - tornado.application - ERROR - Uncaught exception GET /status/ws (::1)\n",
      "HTTPServerRequest(protocol='http', host='localhost:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='::1')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\tornado\\websocket.py\", line 937, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\tornado\\web.py\", line 3290, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\bokeh\\server\\views\\ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired.\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired.\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-733' coro=<Client._gather.<locals>.wait() done, defined at c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\client.py:2209> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\client.py\", line 2218, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-732' coro=<Client._gather.<locals>.wait() done, defined at c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\client.py:2209> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\client.py\", line 2218, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m result \u001b[38;5;241m=\u001b[39m ddf\u001b[38;5;241m.\u001b[39mmap_partitions(process_chunk)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Compute the result\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m computed_result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#computed_result.to_csv(os.path.join(working_dir[:-1], 'dup_diag_unique_encounters.csv'), index=False)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Remove duplicates again to ensure global uniqueness\u001b[39;00m\n\u001b[0;32m     45\u001b[0m computed_result \u001b[38;5;241m=\u001b[39m computed_result\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask\\base.py:342\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask\\base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import config\n",
    "\n",
    "# Configure Dask\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,              # Number of workers\n",
    "    threads_per_worker=1,     # Number of threads per worker\n",
    "    memory_limit='7.50GB',       # Memory limit per worker\n",
    "    processes=True,           # Use separate processes for each worker\n",
    "    dashboard_address=':8787' # Dashboard address http://localhost:8787\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "\n",
    "config.set({'distributed.worker.memory.target': 0.50,    # Spill to disk at 60% memory usage\n",
    "            'distributed.worker.memory.spill': 0.60,     # Spill to disk at 70% memory usage\n",
    "            'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "            'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "            'distributed.scheduler.allowed-failures': 4, # Set the allowed failures to 10          \n",
    "           })\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Repartition the dataframe into smaller chunks\n",
    "num_partitions = 2\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "computed_result = result.compute()\n",
    "#computed_result.to_csv(os.path.join(working_dir[:-1], 'dup_diag_unique_encounters.csv'), index=False)\n",
    "# Remove duplicates again to ensure global uniqueness\n",
    "computed_result = computed_result.drop_duplicates()\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file\n",
    "computed_result.to_csv(output_csv_path, single_file=True, index=False)\n",
    "\n",
    "print(f\"Combined CSV saved to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proc\n",
    "# Define the HDF5 paths\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_proc_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "\n",
    "ddf.to_csv(output_hdf_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds \n",
    "# Define the HDF5 paths\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "\n",
    "ddf.to_csv(output_hdf_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Screens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-89' coro=<Client._gather.<locals>.wait() done, defined at /Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/client.py:2197> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/client.py\", line 2206, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m intersection_ddf \u001b[38;5;241m=\u001b[39m diag_ddf\u001b[38;5;241m.\u001b[39mmerge(vitals_ddf, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencounter_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Write the result to a single CSV file\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m intersection_ddf\u001b[38;5;241m.\u001b[39mto_csv(output_csv_path, single_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mdiag_unique_encounters = dd.read_hdf(diag_path, 'unique_encounters')\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mvitals_unique_encounters = dd.read_hdf(vitals_path, 'unique_encounters')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mintersection_pd.to_csv(output_csv_path, index=False)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_collection.py:2425\u001b[0m, in \u001b[0;36mFrameBase.to_csv\u001b[0;34m(self, filename, **kwargs)\u001b[0m\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See dd.to_csv docstring for more information\"\"\"\u001b[39;00m\n\u001b[1;32m   2423\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_expr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_csv\n\u001b[0;32m-> 2425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_csv(\u001b[38;5;28mself\u001b[39m, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/io/csv.py:274\u001b[0m, in \u001b[0;36mto_csv\u001b[0;34m(df, filename, single_file, encoding, mode, name_function, compression, compute, scheduler, storage_options, header_first_partition_only, compute_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mStore Dask DataFrame to CSV files\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mfsspec.open_files\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_csv \u001b[38;5;28;01mas\u001b[39;00m _to_csv\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _to_csv(\n\u001b[1;32m    275\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_legacy_dataframe(),\n\u001b[1;32m    276\u001b[0m     filename,\n\u001b[1;32m    277\u001b[0m     single_file\u001b[38;5;241m=\u001b[39msingle_file,\n\u001b[1;32m    278\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    279\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    280\u001b[0m     name_function\u001b[38;5;241m=\u001b[39mname_function,\n\u001b[1;32m    281\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    282\u001b[0m     compute\u001b[38;5;241m=\u001b[39mcompute,\n\u001b[1;32m    283\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m    284\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    285\u001b[0m     header_first_partition_only\u001b[38;5;241m=\u001b[39mheader_first_partition_only,\n\u001b[1;32m    286\u001b[0m     compute_kwargs\u001b[38;5;241m=\u001b[39mcompute_kwargs,\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    288\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/dataframe/io/csv.py:1005\u001b[0m, in \u001b[0;36mto_csv\u001b[0;34m(df, filename, single_file, encoding, mode, name_function, compression, compute, scheduler, storage_options, header_first_partition_only, compute_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         compute_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m scheduler\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(dask\u001b[38;5;241m.\u001b[39mcompute(\u001b[38;5;241m*\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompute_kwargs))\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#IT WORKS!!!!\n",
    "# Ambulatory\n",
    "# Read data from HDF5 files using Dask\n",
    "diag_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'amb_unique_encounters.csv')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6pc 8mac cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "\n",
    "# Read the data using Dask\n",
    "diag_ddf = dd.read_csv(diag_path, usecols=['encounter_id'])\n",
    "vitals_ddf = dd.read_csv(vitals_path, usecols=['encounter_id'])\n",
    "\n",
    "# Compute the intersection of 'unique_encounters' columns\n",
    "intersection_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "\n",
    "# Write the result to a single CSV file\n",
    "intersection_ddf.to_csv(output_csv_path, single_file=True, index=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "diag_unique_encounters = dd.read_hdf(diag_path, 'unique_encounters')\n",
    "vitals_unique_encounters = dd.read_hdf(vitals_path, 'unique_encounters')\n",
    "\n",
    "# Compute the intersection\n",
    "intersection = dd.merge(diag_unique_encounters, vitals_unique_encounters, how='inner', on='unique_encounters').drop_duplicates()\n",
    "\n",
    "# Compute and convert to Pandas (if the final result fits into memory), then save\n",
    "intersection_pd = intersection.compute()\n",
    "intersection_pd.to_csv(output_csv_path, index=False)\n",
    "\"\"\"\n",
    "#with pd.HDFStore('amb_unique_encounters.h5', 'w') as store:\n",
    "#    store.put('unique_encounters', intersection_pd, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 07:46:21,723 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:59745 -> tcp://127.0.0.1:59748\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 55] No buffer space available\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:59745 remote=tcp://127.0.0.1:59763>: OSError: [Errno 55] No buffer space available\n",
      "2024-05-19 07:46:36,870 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59745\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59763 remote=tcp://127.0.0.1:59745>: Stream is closed\n",
      "2024-05-19 07:53:13,410 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:59746 (pid=11439) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 07:53:13,543 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59746\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59781 remote=tcp://127.0.0.1:59746>: Stream is closed\n",
      "2024-05-19 07:53:13,545 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59746\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59783 remote=tcp://127.0.0.1:59746>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-05-19 07:53:13,556 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:59746' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('dropna-9c7a13afac0857ff2f8bd429b108010b', 12), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 12), ('max-99b14bffc9941958469a4daa6f1757e3', 13), ('min-f902b508cde289382654148da3092702', 13), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 13), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 13), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 28)} (stimulus_id='handle-worker-cleanup-1716130393.5550091')\n",
      "2024-05-19 07:53:13,555 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:59745 -> tcp://127.0.0.1:59746\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:59745 remote=tcp://127.0.0.1:59778>: BrokenPipeError: [Errno 32] Broken pipe\n",
      "2024-05-19 07:53:13,562 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 07:53:25,943 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59746\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59782 remote=tcp://127.0.0.1:59746>: Stream is closed\n",
      "2024-05-19 07:54:50,020 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:59747 (pid=11441) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 07:54:50,190 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:59747' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('min-f902b508cde289382654148da3092702', 14), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 18), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 8), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 5), ('read_csv-05dadee8513380edf9415ddb9a99163f', 22), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 5), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 14), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 14), ('max-99b14bffc9941958469a4daa6f1757e3', 14), ('min-f902b508cde289382654148da3092702', 5)} (stimulus_id='handle-worker-cleanup-1716130490.190239')\n",
      "2024-05-19 07:54:50,188 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59747\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59790 remote=tcp://127.0.0.1:59747>: Stream is closed\n",
      "2024-05-19 07:54:50,199 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 07:55:20,112 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59747\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59794 remote=tcp://127.0.0.1:59747>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-05-19 07:55:24,532 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59747\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59793 remote=tcp://127.0.0.1:59747>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-05-19 08:31:36,481 - distributed.scheduler - WARNING - Worker failed to heartbeat for 1890s; attempting restart: <WorkerState 'tcp://127.0.0.1:59745', name: 0, status: running, memory: 13, processing: 4>\n",
      "2024-05-19 08:31:36,751 - distributed.scheduler - WARNING - Worker failed to heartbeat for 1879s; attempting restart: <WorkerState 'tcp://127.0.0.1:59748', name: 3, status: running, memory: 9, processing: 7>\n",
      "2024-05-19 08:31:36,766 - distributed.scheduler - WARNING - Worker failed to heartbeat for 1894s; attempting restart: <WorkerState 'tcp://127.0.0.1:59796', name: 2, status: running, memory: 3, processing: 3>\n",
      "2024-05-19 08:31:40,994 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-19 08:31:41,119 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-19 08:31:41,124 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-19 08:31:41,534 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:59745' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('min-f902b508cde289382654148da3092702', 8), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 12), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 8), ('min-f902b508cde289382654148da3092702', 11), ('max-99b14bffc9941958469a4daa6f1757e3', 13), ('max-99b14bffc9941958469a4daa6f1757e3', 8), ('read_csv-05dadee8513380edf9415ddb9a99163f', 9), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 4), ('min-f902b508cde289382654148da3092702', 13), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 13), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 13), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 6), ('max-99b14bffc9941958469a4daa6f1757e3', 11)} (stimulus_id='handle-worker-cleanup-1716132701.505439')\n",
      "2024-05-19 08:31:41,805 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:59796' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('read_csv-05dadee8513380edf9415ddb9a99163f', 14), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 14), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 22)} (stimulus_id='handle-worker-cleanup-1716132701.80511')\n",
      "2024-05-19 08:31:41,809 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:59748' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('max-99b14bffc9941958469a4daa6f1757e3', 10), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 3, 2), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 11), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 1), ('max-99b14bffc9941958469a4daa6f1757e3', 9), ('min-f902b508cde289382654148da3092702', 9), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 3), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 10), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 0)} (stimulus_id='handle-worker-cleanup-1716132701.809015')\n",
      "2024-05-19 08:31:41,966 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 08:31:42,011 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 08:31:42,100 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 08:31:42,411 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59796\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59982 remote=tcp://127.0.0.1:59796>: Stream is closed\n",
      "2024-05-19 08:31:42,419 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:59786 -> tcp://127.0.0.1:59796\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:59786 remote=tcp://127.0.0.1:59800>: BrokenPipeError: [Errno 32] Broken pipe\n",
      "2024-05-19 08:32:50,253 - tornado.application - ERROR - Uncaught exception GET /status/ws (::1)\n",
      "HTTPServerRequest(protocol='http', host='localhost:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='::1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/websocket.py\", line 937, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/web.py\", line 3290, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired.\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired.\n",
      "2024-05-19 08:58:24,542 - distributed.scheduler - WARNING - Worker failed to heartbeat for 1386s; attempting restart: <WorkerState 'tcp://127.0.0.1:60161', name: 2, status: running, memory: 4, processing: 4>\n",
      "2024-05-19 08:58:28,618 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-19 08:58:28,942 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:60161' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('read_csv-05dadee8513380edf9415ddb9a99163f', 18), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 20), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 21), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 22)} (stimulus_id='handle-worker-cleanup-1716134308.941659')\n",
      "2024-05-19 08:58:28,969 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 08:58:28,941 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60161\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60192 remote=tcp://127.0.0.1:60161>: Stream is closed\n",
      "2024-05-19 08:58:31,055 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:60163 -> tcp://127.0.0.1:60161\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:60163 remote=tcp://127.0.0.1:60191>: BrokenPipeError: [Errno 32] Broken pipe\n",
      "2024-05-19 08:58:42,750 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60161\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60227 remote=tcp://127.0.0.1:60161>: Stream is closed\n",
      "2024-05-19 09:01:53,119 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:60162 (pid=11842) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 09:01:53,265 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:60162' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('dropna-9c7a13afac0857ff2f8bd429b108010b', 12), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 11), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 17), ('read_csv-05dadee8513380edf9415ddb9a99163f', 9), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 14), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 14), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 19)} (stimulus_id='handle-worker-cleanup-1716134513.26417')\n",
      "2024-05-19 09:01:53,279 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 09:01:55,516 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:59786 -> tcp://127.0.0.1:60162\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:59786 remote=tcp://127.0.0.1:60388>: BrokenPipeError: [Errno 32] Broken pipe\n",
      "2024-05-19 09:05:18,507 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:59786 (pid=11616) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 09:05:18,706 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59786\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60425 remote=tcp://127.0.0.1:59786>: Stream is closed\n",
      "2024-05-19 09:05:18,707 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59786\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 230, in read\n",
      "    buffer = await read_bytes_rw(stream, buffer_nbytes)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60470 remote=tcp://127.0.0.1:59786>: Stream is closed\n",
      "2024-05-19 09:05:18,756 - distributed.scheduler - ERROR - Task ('min-f902b508cde289382654148da3092702', 14) marked as failed because 4 workers died while trying to run it\n",
      "2024-05-19 09:05:18,757 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:59786' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('min-f902b508cde289382654148da3092702', 8), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 2), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 8), ('min-f902b508cde289382654148da3092702', 7), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 4), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 10), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 23), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 7), ('max-99b14bffc9941958469a4daa6f1757e3', 15), ('read_csv-05dadee8513380edf9415ddb9a99163f', 8), ('read_csv-05dadee8513380edf9415ddb9a99163f', 20), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 7), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 16), ('max-99b14bffc9941958469a4daa6f1757e3', 8), ('min-f902b508cde289382654148da3092702', 15), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 15), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 5)} (stimulus_id='handle-worker-cleanup-1716134718.745339')\n",
      "2024-05-19 09:05:18,729 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:59786\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 297, in write\n",
      "    raise StreamClosedError()\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2877, in get_data_from_worker\n",
      "    await comm.write(\"OK\")\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 307, in write\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60407 remote=tcp://127.0.0.1:59786>: Stream is closed\n",
      "2024-05-19 09:05:18,774 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 09:07:51,419 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:60465 (pid=12083) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 09:07:51,574 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:60465' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('read_csv-05dadee8513380edf9415ddb9a99163f', 16), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 17), ('max-99b14bffc9941958469a4daa6f1757e3', 6), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 10), ('repartitiontofewer-f00dd63eba240977074f1c6db7078f37', 3), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 13), ('min-f902b508cde289382654148da3092702', 6), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 6), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 19), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 9)} (stimulus_id='handle-worker-cleanup-1716134871.573523')\n",
      "2024-05-19 09:07:51,600 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 09:07:51,603 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60465\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60514 remote=tcp://127.0.0.1:60465>: Stream is closed\n",
      "2024-05-19 09:08:04,668 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:60389 -> tcp://127.0.0.1:60465\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 297, in write\n",
      "    raise StreamClosedError()\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1782, in get_data\n",
      "    compressed = await comm.write(msg, serializers=serializers)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 307, in write\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:60389 remote=tcp://127.0.0.1:60508>: Stream is closed\n",
      "2024-05-19 09:08:04,686 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60465\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60519 remote=tcp://127.0.0.1:60465>: Stream is closed\n",
      "2024-05-19 09:08:37,686 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60465\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60507 remote=tcp://127.0.0.1:60465>: Stream is closed\n",
      "2024-05-19 11:03:21,727 - distributed.scheduler - WARNING - Worker failed to heartbeat for 1679s; attempting restart: <WorkerState 'tcp://127.0.0.1:60389', name: 2, status: running, memory: 12, processing: 2>\n",
      "2024-05-19 11:03:21,760 - distributed.scheduler - WARNING - Worker failed to heartbeat for 1717s; attempting restart: <WorkerState 'tcp://127.0.0.1:60509', name: 1, status: running, memory: 4, processing: 3>\n",
      "2024-05-19 11:03:25,808 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-19 11:03:25,856 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-19 11:03:26,300 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:60509' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('dropna-9c7a13afac0857ff2f8bd429b108010b', 10), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 15), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 15), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 16)} (stimulus_id='handle-worker-cleanup-1716141806.2984169')\n",
      "2024-05-19 11:03:26,380 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 11:03:26,401 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60389\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 230, in read\n",
      "    buffer = await read_bytes_rw(stream, buffer_nbytes)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60533 remote=tcp://127.0.0.1:60389>: Stream is closed\n",
      "2024-05-19 11:03:26,488 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:60389' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('read_csv-05dadee8513380edf9415ddb9a99163f', 13), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 18), ('max-99b14bffc9941958469a4daa6f1757e3', 0), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 11), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 14), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 14), ('min-f902b508cde289382654148da3092702', 0), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 7), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 0, 0), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 3), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 0), ('max-99b14bffc9941958469a4daa6f1757e3', 14)} (stimulus_id='handle-worker-cleanup-1716141806.4869802')\n",
      "2024-05-19 11:03:26,510 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 11:03:26,703 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60389\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60471 remote=tcp://127.0.0.1:60389>: Stream is closed\n",
      "2024-05-19 12:40:09,635 - distributed.worker.memory - WARNING - Worker is at 94% memory usage. Pausing worker.  Process memory: 1.76 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-05-19 12:40:23,552 - distributed.worker.memory - WARNING - Worker is at 27% memory usage. Resuming worker. Process memory: 515.98 MiB -- Worker memory limit: 1.86 GiB\n",
      "2024-05-19 12:45:47,318 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:60522 (pid=12133) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 12:45:47,413 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:60522' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('dropna-9c7a13afac0857ff2f8bd429b108010b', 8), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 6), ('max-99b14bffc9941958469a4daa6f1757e3', 6), ('max-99b14bffc9941958469a4daa6f1757e3', 12), ('min-f902b508cde289382654148da3092702', 9), ('min-f902b508cde289382654148da3092702', 6), ('min-f902b508cde289382654148da3092702', 12), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 0), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 19), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 9)} (stimulus_id='handle-worker-cleanup-1716147947.411804')\n",
      "2024-05-19 12:45:47,397 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60522\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61180 remote=tcp://127.0.0.1:60522>: Stream is closed\n",
      "2024-05-19 12:45:47,395 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60522\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61599 remote=tcp://127.0.0.1:60522>: Stream is closed\n",
      "2024-05-19 12:45:47,439 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 12:46:32,564 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:60163 -> tcp://127.0.0.1:60522\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:60163 remote=tcp://127.0.0.1:60805>: BrokenPipeError: [Errno 32] Broken pipe\n",
      "2024-05-19 12:49:44,653 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60833\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/utils.py\", line 1956, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 546, in connect\n",
      "    stream = await self.client.connect(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/tcpclient.py\", line 279, in connect\n",
      "    af, addr, stream = await connector.start(connect_timeout=timeout)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/utils.py\", line 1955, in wait_for\n",
      "    async with asyncio.timeout(timeout):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/asyncio/timeouts.py\", line 115, in __aexit__\n",
      "    raise TimeoutError from exc_val\n",
      "TimeoutError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2860, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1620, in connect\n",
      "    return await self._connect(addr=addr, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1564, in _connect\n",
      "    comm = await connect(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:60833 after 30 s\n",
      "2024-05-19 12:50:02,964 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 1.57 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-05-19 12:51:22,403 - distributed.worker.memory - WARNING - Worker is at 58% memory usage. Resuming worker. Process memory: 1.09 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-05-19 12:52:06,617 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:60834 (pid=12238) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 12:52:06,755 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60834\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 230, in read\n",
      "    buffer = await read_bytes_rw(stream, buffer_nbytes)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61615 remote=tcp://127.0.0.1:60834>: Stream is closed\n",
      "2024-05-19 12:52:06,756 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60834\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61605 remote=tcp://127.0.0.1:60834>: Stream is closed\n",
      "2024-05-19 12:52:06,771 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:60834' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('min-f902b508cde289382654148da3092702', 11), ('max-99b14bffc9941958469a4daa6f1757e3', 0), ('min-f902b508cde289382654148da3092702', 7), ('max-99b14bffc9941958469a4daa6f1757e3', 3), ('min-f902b508cde289382654148da3092702', 0), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 10), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 7), ('min-f902b508cde289382654148da3092702', 3), ('min-f902b508cde289382654148da3092702', 9), ('max-99b14bffc9941958469a4daa6f1757e3', 9), ('max-99b14bffc9941958469a4daa6f1757e3', 14), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 3, 0), ('max-99b14bffc9941958469a4daa6f1757e3', 11)} (stimulus_id='handle-worker-cleanup-1716148326.770527')\n",
      "2024-05-19 12:52:06,793 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 12:55:59,306 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:60163 (pid=11841) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 12:55:59,384 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:60163' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('max-99b14bffc9941958469a4daa6f1757e3', 10), ('min-f902b508cde289382654148da3092702', 1), ('max-99b14bffc9941958469a4daa6f1757e3', 13), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 2, 3), ('min-f902b508cde289382654148da3092702', 13), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 4), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 3), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 9), ('max-99b14bffc9941958469a4daa6f1757e3', 1), ('max-99b14bffc9941958469a4daa6f1757e3', 7)} (stimulus_id='handle-worker-cleanup-1716148559.384363')\n",
      "2024-05-19 12:55:59,371 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60163\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61634 remote=tcp://127.0.0.1:60163>: Stream is closed\n",
      "2024-05-19 12:55:59,436 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 12:56:01,091 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:61601 -> tcp://127.0.0.1:60163\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:61601 remote=tcp://127.0.0.1:61614>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-05-19 12:56:02,220 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:61601 (pid=12464) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 12:56:02,333 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:61601' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('dropna-9c7a13afac0857ff2f8bd429b108010b', 2), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 21), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 17), ('max-99b14bffc9941958469a4daa6f1757e3', 6), ('max-99b14bffc9941958469a4daa6f1757e3', 12), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 4), ('min-f902b508cde289382654148da3092702', 0), ('min-f902b508cde289382654148da3092702', 6), ('min-f902b508cde289382654148da3092702', 12), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 16), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 0), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 5)} (stimulus_id='handle-worker-cleanup-1716148562.332823')\n",
      "2024-05-19 12:56:02,341 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 12:56:03,945 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:60833 -> tcp://127.0.0.1:61601\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:60833 remote=tcp://127.0.0.1:61629>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-05-19 12:56:03,952 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:61601\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61626 remote=tcp://127.0.0.1:61601>: Stream is closed\n",
      "2024-05-19 13:11:30,818 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:61618 (pid=12524) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 13:11:30,992 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:61618\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61633 remote=tcp://127.0.0.1:61618>: Stream is closed\n",
      "2024-05-19 13:11:31,008 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:61618' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 7), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 2), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 2), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 20)} (stimulus_id='handle-worker-cleanup-1716149491.006321')\n",
      "2024-05-19 13:11:30,992 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:61618\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:62835 remote=tcp://127.0.0.1:61618>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-05-19 13:11:31,018 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:61639 -> tcp://127.0.0.1:61618\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:61639 remote=tcp://127.0.0.1:61686>: BrokenPipeError: [Errno 32] Broken pipe\n",
      "2024-05-19 13:11:31,033 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 13:11:31,915 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:61639 (pid=12539) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 13:11:31,989 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:61639\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61870 remote=tcp://127.0.0.1:61639>: Stream is closed\n",
      "2024-05-19 13:11:31,999 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:61639' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('max-99b14bffc9941958469a4daa6f1757e3', 12), ('min-f902b508cde289382654148da3092702', 12), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 6), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 6), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 9), ('read_csv-05dadee8513380edf9415ddb9a99163f', 7)} (stimulus_id='handle-worker-cleanup-1716149491.999792')\n",
      "2024-05-19 13:11:32,004 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 13:15:40,209 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:63095\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/utils.py\", line 1956, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 546, in connect\n",
      "    stream = await self.client.connect(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/tcpclient.py\", line 279, in connect\n",
      "    af, addr, stream = await connector.start(connect_timeout=timeout)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/utils.py\", line 1955, in wait_for\n",
      "    async with asyncio.timeout(timeout):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/asyncio/timeouts.py\", line 115, in __aexit__\n",
      "    raise TimeoutError from exc_val\n",
      "TimeoutError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2860, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1620, in connect\n",
      "    return await self._connect(addr=addr, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1564, in _connect\n",
      "    comm = await connect(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:63095 after 30 s\n",
      "2024-05-19 13:15:46,804 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:60833 (pid=12237) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 13:15:46,991 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:60833' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('max-99b14bffc9941958469a4daa6f1757e3', 0), ('min-f902b508cde289382654148da3092702', 0), ('max-99b14bffc9941958469a4daa6f1757e3', 15), ('min-f902b508cde289382654148da3092702', 15), ('repartitiontofewer-f00dd63eba240977074f1c6db7078f37', 5), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 4, 1), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 11), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 0, 0), ('max-99b14bffc9941958469a4daa6f1757e3', 2), ('max-99b14bffc9941958469a4daa6f1757e3', 8), ('min-f902b508cde289382654148da3092702', 2), ('max-99b14bffc9941958469a4daa6f1757e3', 14), ('min-f902b508cde289382654148da3092702', 8), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 1), ('repartitiontofewer-f00dd63eba240977074f1c6db7078f37', 4), ('max-99b14bffc9941958469a4daa6f1757e3', 10), ('min-f902b508cde289382654148da3092702', 7), ('min-f902b508cde289382654148da3092702', 10), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 0)} (stimulus_id='handle-worker-cleanup-1716149746.988756')\n",
      "2024-05-19 13:15:46,974 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:60833\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:63367 remote=tcp://127.0.0.1:60833>: Stream is closed\n",
      "2024-05-19 13:15:47,031 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 13:43:40,206 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:61636 (pid=12535) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 13:43:40,314 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:61636' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('dropna-9c7a13afac0857ff2f8bd429b108010b', 24), ('max-99b14bffc9941958469a4daa6f1757e3', 13), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 12), ('max-99b14bffc9941958469a4daa6f1757e3', 3), ('min-f902b508cde289382654148da3092702', 13), ('read_csv-05dadee8513380edf9415ddb9a99163f', 18), ('max-99b14bffc9941958469a4daa6f1757e3', 9), ('min-f902b508cde289382654148da3092702', 9), ('min-f902b508cde289382654148da3092702', 3), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 13), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 7), ('max-99b14bffc9941958469a4daa6f1757e3', 2), ('min-f902b508cde289382654148da3092702', 2), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 2, 1), ('read_csv-05dadee8513380edf9415ddb9a99163f', 17), ('max-99b14bffc9941958469a4daa6f1757e3', 7)} (stimulus_id='handle-worker-cleanup-1716151420.313133')\n",
      "2024-05-19 13:43:40,296 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:61636\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:64452 remote=tcp://127.0.0.1:61636>: Stream is closed\n",
      "2024-05-19 13:43:40,298 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:65055 -> tcp://127.0.0.1:61636\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:65055 remote=tcp://127.0.0.1:50238>: BrokenPipeError: [Errno 32] Broken pipe\n",
      "2024-05-19 13:43:40,335 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 13:43:54,642 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:61636\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:65401 remote=tcp://127.0.0.1:61636>: Stream is closed\n",
      "2024-05-19 13:44:08,715 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:63100 (pid=12671) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 13:44:08,813 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:63100\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50359 remote=tcp://127.0.0.1:63100>: Stream is closed\n",
      "2024-05-19 13:44:08,822 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:63100' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('min-f902b508cde289382654148da3092702', 8), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 8), ('min-f902b508cde289382654148da3092702', 11), ('min-f902b508cde289382654148da3092702', 1), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 11), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 1), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 5), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 1), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 14), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 16), ('min-f902b508cde289382654148da3092702', 5), ('max-99b14bffc9941958469a4daa6f1757e3', 8), ('max-99b14bffc9941958469a4daa6f1757e3', 11)} (stimulus_id='handle-worker-cleanup-1716151448.8219352')\n",
      "2024-05-19 13:44:08,834 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 13:44:10,315 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:63100\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50354 remote=tcp://127.0.0.1:63100>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-05-19 13:55:40,817 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:63095 (pid=12669) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 13:55:40,918 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:63095' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('dropna-9c7a13afac0857ff2f8bd429b108010b', 12), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 2), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 8), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 2, 7), ('max-99b14bffc9941958469a4daa6f1757e3', 0), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 11), ('min-f902b508cde289382654148da3092702', 15), ('repartitiontofewer-f00dd63eba240977074f1c6db7078f37', 7), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 1), ('min-f902b508cde289382654148da3092702', 0), ('max-99b14bffc9941958469a4daa6f1757e3', 12), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 10), ('max-99b14bffc9941958469a4daa6f1757e3', 15), ('min-f902b508cde289382654148da3092702', 12), ('max-99b14bffc9941958469a4daa6f1757e3', 14), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 19)} (stimulus_id='handle-worker-cleanup-1716152140.9149618')\n",
      "2024-05-19 13:55:40,898 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:50355 -> tcp://127.0.0.1:63095\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:50355 remote=tcp://127.0.0.1:50416>: BrokenPipeError: [Errno 32] Broken pipe\n",
      "2024-05-19 13:55:40,948 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 13:56:28,941 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 1.53 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-05-19 13:56:34,811 - distributed.worker.memory - WARNING - Worker is at 20% memory usage. Resuming worker. Process memory: 392.09 MiB -- Worker memory limit: 1.86 GiB\n",
      "2024-05-19 14:04:07,116 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:50466 (pid=12868) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 14:04:07,201 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:50466\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distribute2024-05-19 14:04:07,218 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:50466' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('dropna-9c7a13afac0857ff2f8bd429b108010b', 14), ('min-f902b508cde289382654148da3092702', 10), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 10), ('max-99b14bffc9941958469a4daa6f1757e3', 15), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 10), ('min-f902b508cde289382654148da3092702', 15), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 15)} (stimulus_id='handle-worker-cleanup-1716152647.217182')\n",
      "d/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50481 remote=tcp://127.0.0.1:50466>: Stream is closed\n",
      "2024-05-19 14:04:07,236 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 14:04:20,900 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:50361 -> tcp://127.0.0.1:50466\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:50361 remote=tcp://127.0.0.1:50477>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-05-19 14:04:41,702 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:50355 (pid=12817) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 14:04:41,796 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:50355\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50480 remote=tcp://127.0.0.1:50355>: Stream is closed\n",
      "2024-05-19 14:04:41,797 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:50355\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2860, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1620, in connect\n",
      "    return await self._connect(addr=addr, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1564, in _connect\n",
      "    comm = await connect(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "                ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:50486 remote=tcp://127.0.0.1:50355>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-05-19 14:04:41,810 - distributed.scheduler - ERROR - Task ('repartitiontofewer-f00dd63eba240977074f1c6db7078f37', 6) marked as failed because 4 workers died while trying to run it\n",
      "2024-05-19 14:04:41,810 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:50355' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 1, 2), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 24), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 2), ('repartitiontofewer-f00dd63eba240977074f1c6db7078f37', 1), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 11), ('getitem-3b3b2cfefeb15789937c27f88cadaa18', 14), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 7), ('repartitionquantiles-a2a6a7a2412799aa76c4fa58f77b77ce', 0, 0), ('dropna-9c7a13afac0857ff2f8bd429b108010b', 16), ('max-99b14bffc9941958469a4daa6f1757e3', 2), ('max-99b14bffc9941958469a4daa6f1757e3', 14)} (stimulus_id='handle-worker-cleanup-1716152681.809032')\n",
      "2024-05-19 14:04:41,822 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-19 14:04:42,084 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:65055 -> tcp://127.0.0.1:50355\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:65055 remote=tcp://127.0.0.1:50360>: BrokenPipeError: [Errno 32] Broken pipe\n",
      "2024-05-19 14:04:43,032 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:65055 (pid=12697) exceeded 95% memory budget. Restarting...\n",
      "2024-05-19 14:04:43,102 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:50361 -> tcp://127.0.0.1:65055\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 1783, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:50361 remote=tcp://127.0.0.1:50482>: Stream is closed\n",
      "2024-05-19 14:04:43,103 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:65055\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/worker.py\", line 2863, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50415 remote=tcp://127.0.0.1:65055>: Stream is closed\n",
      "2024-05-19 14:04:43,916 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task ('repartitiontofewer-f00dd63eba240977074f1c6db7078f37', 6) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:50355. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Merge DataFrames sequentially with intermediate persisting\u001b[39;00m\n\u001b[1;32m     60\u001b[0m merged \u001b[38;5;241m=\u001b[39m diag_ddf\u001b[38;5;241m.\u001b[39mmerge(vitals_ddf, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencounter_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m merged \u001b[38;5;241m=\u001b[39m merged\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[1;32m     63\u001b[0m merged \u001b[38;5;241m=\u001b[39m merged\u001b[38;5;241m.\u001b[39mmerge(med_ddf, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencounter_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m merged \u001b[38;5;241m=\u001b[39m merged\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_collection.py:446\u001b[0m, in \u001b[0;36mFrameBase.persist\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpersist\u001b[39m(\u001b[38;5;28mself\u001b[39m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 446\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mpersist(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_collection.py:590\u001b[0m, in \u001b[0;36mFrameBase.optimize\u001b[0;34m(self, fuse)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, fuse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    573\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimizes the DataFrame.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m    Runs the optimizer with all steps over the DataFrame and wraps the result in a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;124;03m        The optimized Dask Dataframe\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_expr.py:94\u001b[0m, in \u001b[0;36mExpr.optimize\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_expr.py:3032\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(expr, fuse)\u001b[0m\n\u001b[1;32m   3011\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"High level query optimization\u001b[39;00m\n\u001b[1;32m   3012\u001b[0m \n\u001b[1;32m   3013\u001b[0m \u001b[38;5;124;03mThis leverages three optimization passes:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;124;03moptimize_blockwise_fusion\u001b[39;00m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3030\u001b[0m stage: core\u001b[38;5;241m.\u001b[39mOptimizerStage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fuse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplified-physical\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimize_until(expr, stage)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_expr.py:2993\u001b[0m, in \u001b[0;36moptimize_until\u001b[0;34m(expr, stage)\u001b[0m\n\u001b[1;32m   2990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr\n\u001b[1;32m   2992\u001b[0m \u001b[38;5;66;03m# Lower\u001b[39;00m\n\u001b[0;32m-> 2993\u001b[0m expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39mlower_completely()\n\u001b[1;32m   2994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphysical\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_core.py:444\u001b[0m, in \u001b[0;36mExpr.lower_completely\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m lowered \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     new \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39mlower_once(lowered)\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m==\u001b[39m expr\u001b[38;5;241m.\u001b[39m_name:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_core.py:399\u001b[0m, in \u001b[0;36mExpr.lower_once\u001b[0;34m(self, lowered)\u001b[0m\n\u001b[1;32m    396\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Lower this node\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m out \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39m_lower()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     out \u001b[38;5;241m=\u001b[39m expr\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_merge.py:352\u001b[0m, in \u001b[0;36mMerge._lower\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m shuffle_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle_method\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# TODO: capture index-merge as well\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m left_already_partitioned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_condition_alread_partitioned(left, left_on)\n\u001b[1;32m    353\u001b[0m right_already_partitioned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_condition_alread_partitioned(\n\u001b[1;32m    354\u001b[0m     right, right_on\n\u001b[1;32m    355\u001b[0m )\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m#  1. Add/leverage partition statistics\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# Check for \"trivial\" broadcast (single partition)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_merge.py:334\u001b[0m, in \u001b[0;36mMerge._on_condition_alread_partitioned\u001b[0;34m(self, expr, on)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_on_condition_alread_partitioned\u001b[39m(\u001b[38;5;28mself\u001b[39m, expr, on):\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(on, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    333\u001b[0m         result \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 334\u001b[0m             on \u001b[38;5;129;01min\u001b[39;00m expr\u001b[38;5;241m.\u001b[39munique_partition_mapping_columns_from_shuffle\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m (on,) \u001b[38;5;129;01min\u001b[39;00m expr\u001b[38;5;241m.\u001b[39munique_partition_mapping_columns_from_shuffle\n\u001b[1;32m    336\u001b[0m         )\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(on) \u001b[38;5;129;01min\u001b[39;00m expr\u001b[38;5;241m.\u001b[39munique_partition_mapping_columns_from_shuffle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_repartition.py:70\u001b[0m, in \u001b[0;36mRepartition.unique_partition_mapping_columns_from_shuffle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_partition_mapping_columns_from_shuffle\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpartitions \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mnpartitions\n\u001b[1;32m     71\u001b[0m     ):\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39munique_partition_mapping_columns_from_shuffle\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_shuffle.py:752\u001b[0m, in \u001b[0;36mBaseSetIndexSortValues.npartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnpartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpartitions\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_divisions()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_shuffle.py:735\u001b[0m, in \u001b[0;36mBaseSetIndexSortValues._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    729\u001b[0m     is_index_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_divisions_column\u001b[38;5;241m.\u001b[39m_meta)\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_divisions_column\u001b[38;5;241m.\u001b[39mknown_divisions\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_divisions_column\u001b[38;5;241m.\u001b[39mnpartitions \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mnpartitions\n\u001b[1;32m    732\u001b[0m ):\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mother\u001b[38;5;241m.\u001b[39mdivisions\n\u001b[0;32m--> 735\u001b[0m divisions, mins, maxes, presorted \u001b[38;5;241m=\u001b[39m _get_divisions(\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe,\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_divisions_column,\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npartitions_input,\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mascending,\n\u001b[1;32m    740\u001b[0m     upsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample,\n\u001b[1;32m    741\u001b[0m )\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m presorted \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npartitions_input:\n\u001b[1;32m    743\u001b[0m     divisions \u001b[38;5;241m=\u001b[39m mins\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;241m+\u001b[39m [maxes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_shuffle.py:1288\u001b[0m, in \u001b[0;36m_get_divisions\u001b[0;34m(frame, other, npartitions, ascending, partition_size, upsample)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m divisions_lru:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m divisions_lru[key]\n\u001b[0;32m-> 1288\u001b[0m result \u001b[38;5;241m=\u001b[39m _calculate_divisions(\n\u001b[1;32m   1289\u001b[0m     frame, other, npartitions, ascending, partition_size, upsample\n\u001b[1;32m   1290\u001b[0m )\n\u001b[1;32m   1291\u001b[0m divisions_lru[key] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_shuffle.py:1312\u001b[0m, in \u001b[0;36m_calculate_divisions\u001b[0;34m(frame, other, npartitions, ascending, partition_size, upsample)\u001b[0m\n\u001b[1;32m   1309\u001b[0m     other \u001b[38;5;241m=\u001b[39m new_collection(other)\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mas_ordered()\u001b[38;5;241m.\u001b[39m_expr\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1312\u001b[0m     divisions, mins, maxes \u001b[38;5;241m=\u001b[39m compute(\n\u001b[1;32m   1313\u001b[0m         new_collection(RepartitionQuantiles(other, npartitions, upsample\u001b[38;5;241m=\u001b[39mupsample)),\n\u001b[1;32m   1314\u001b[0m         new_collection(other)\u001b[38;5;241m.\u001b[39mmap_partitions(M\u001b[38;5;241m.\u001b[39mmin),\n\u001b[1;32m   1315\u001b[0m         new_collection(other)\u001b[38;5;241m.\u001b[39mmap_partitions(M\u001b[38;5;241m.\u001b[39mmax),\n\u001b[1;32m   1316\u001b[0m     )\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;66;03m# When there are nulls and a column is non-numeric, a TypeError is sometimes raised as a result of\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# 1) computing mins/maxes above, 2) every null being switched to NaN, and 3) NaN being a float.\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;66;03m# Also, Pandas ExtensionDtypes may cause TypeErrors when dealing with special nulls such as pd.NaT or pd.NA.\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# If this happens, we hint the user about eliminating nulls beforehand.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_numeric_dtype(other\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mdtype):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/distributed/client.py:2232\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2230\u001b[0m         exc \u001b[38;5;241m=\u001b[39m CancelledError(key)\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKilledWorker\u001b[0m: Attempted to run task ('repartitiontofewer-f00dd63eba240977074f1c6db7078f37', 6) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:50355. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:20:26,024 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 7 memory: 44 MB fds: 172>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/tornado/ioloop.py\", line 919, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/distributed/system_monitor.py\", line 168, in update\n",
      "    net_ioc = psutil.net_io_counters()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/blocke/anaconda3/lib/python3.11/site-packages/psutil/__init__.py\", line 2117, in net_io_counters\n",
      "    rawdict = _psplatform.net_io_counters()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 12] Cannot allocate memory\n"
     ]
    }
   ],
   "source": [
    "# Read data from HDF5 files using Dask\n",
    "diag_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "med_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv')\n",
    "proc_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv')\n",
    "lab_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv') \n",
    "\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'inp_unique_encounters.csv')\n",
    "\n",
    "# Commented out if prior code block still running\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6pc 8mac cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "# Read the data using Dask and ensure 'encounter_id' is read as a string\n",
    "diag_ddf = dd.read_csv(diag_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).dropna(subset=['encounter_id'])\n",
    "vitals_ddf = dd.read_csv(vitals_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).dropna(subset=['encounter_id'])\n",
    "med_ddf = dd.read_csv(med_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).dropna(subset=['encounter_id'])\n",
    "lab_ddf = dd.read_csv(lab_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).dropna(subset=['encounter_id'])\n",
    "proc_ddf = dd.read_csv(proc_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).dropna(subset=['encounter_id'])\n",
    "\n",
    "# Repartition the DataFrames into smaller chunks for better parallelism\n",
    "num_partitions = 16\n",
    "diag_ddf = diag_ddf.repartition(npartitions=num_partitions)\n",
    "vitals_ddf = vitals_ddf.repartition(npartitions=num_partitions)\n",
    "med_ddf = med_ddf.repartition(npartitions=num_partitions)\n",
    "lab_ddf = lab_ddf.repartition(npartitions=num_partitions)\n",
    "proc_ddf = proc_ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Ensure consistent partitioning and indexing\n",
    "def set_index_and_repartition(ddf):\n",
    "    ddf = ddf.set_index('encounter_id').repartition(npartitions=num_partitions)\n",
    "    return ddf\n",
    "\n",
    "diag_ddf = set_index_and_repartition(diag_ddf)\n",
    "vitals_ddf = set_index_and_repartition(vitals_ddf)\n",
    "med_ddf = set_index_and_repartition(med_ddf)\n",
    "lab_ddf = set_index_and_repartition(lab_ddf)\n",
    "proc_ddf = set_index_and_repartition(proc_ddf)\n",
    "\n",
    "# Merge DataFrames sequentially with intermediate persisting\n",
    "merged = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(med_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(proc_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(lab_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "# Drop duplicates just in case\n",
    "merged = merged.drop_duplicates()\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file directly with Dask\n",
    "merged.to_csv(output_csv_path, single_file=True, index=False)\n",
    "\n",
    "print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "def print_structure(name, obj):\n",
    "    print(name)\n",
    "\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "\n",
    "# Check structure in diag_unique_encounters.h5\n",
    "print(\"Structure of diag_unique_encounters.h5:\")\n",
    "with h5py.File(diag_path, 'r') as f:\n",
    "    f.visititems(print_structure)\n",
    "\n",
    "# Check structure in vitals_unique_encounters.h5\n",
    "print(\"Structure of vitals_unique_encounters.h5:\")\n",
    "with h5py.File(vitals_path, 'r') as f:\n",
    "    f.visititems(print_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambulatory 2\n",
    "# Read data from HDF5 files using Dask\n",
    "\n",
    "# Define the paths\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'amb_unique_encounters2.csv')\n",
    "\n",
    "# Read data from HDF5 files using Dask\n",
    "diag_unique_encounters = dd.read_hdf(diag_path, 'unique_encounters/table')  # Update the key as needed\n",
    "vitals_unique_encounters = dd.read_hdf(vitals_path, 'unique_encounters/table')  # Update the key as needed\n",
    "\n",
    "# Compute the intersection\n",
    "intersection = dd.merge(diag_unique_encounters, vitals_unique_encounters, how='inner', on='unique_encounters').drop_duplicates()\n",
    "\n",
    "# Compute and convert to Pandas (if the final result fits into memory), then save\n",
    "intersection_pd = intersection.compute()\n",
    "intersection_pd.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "#with pd.HDFStore('amb_unique_encounters.h5', 'w') as store:\n",
    "#    store.put('unique_encounters', intersection_pd, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vital Signs - Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "start_time = time.time()\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 853\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"date\",\"value\",\"text_value\",\"units_of_measure\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "unique_chunks = []\n",
    "for i in range(1, num_spreadsheets+1):\n",
    "    print(f'{i:04}')\n",
    "    vital_signs = pd.read_csv(database_dir + r\"Vital Signs\\vital_signs\"+f'{i:04}'+\".csv\",\n",
    "                            names = columns,\n",
    "                            usecols = [\"encounter_id\"],\n",
    "                            dtype = {\"encounter_id\":str})\n",
    "    unique_chunk = vital_signs[\"encounter_id\"].unique().tolist()\n",
    "    unique_chunks.append(unique_chunk)\n",
    "    del unique_chunk\n",
    "unique_vs_encounters = list(set([item for sublist in unique_chunks for item in sublist]))\n",
    "print(\"Vitals\", len(unique_vs_encounters))\n",
    "\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"vitals_encounters.csv\")\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"encounter_id\"])  # Write the header\n",
    "    for encounter_id in unique_vs_encounters:\n",
    "        writer.writerow([encounter_id])\n",
    "print(f\"Unique encounter IDs with vital signs reported are written to {output_file}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vital signs - to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 853\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"date\",\"value\",\"text_value\",\"units_of_measure\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "unique_vs_encounters = set()\n",
    "\n",
    "for i in range(1, num_spreadsheets + 1):\n",
    "    print(f'{i:04}')  \n",
    "    vital_signs = pd.read_csv(\n",
    "        database_dir + r\"Vital Signs\\vital_signs\" + f'{i:04}' + \".csv\",\n",
    "        names=columns,          # Override column names\n",
    "        usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "        dtype={\"encounter_id\": str}  # Ensure \"encounter_id\" is read as a string\n",
    "    )\n",
    "    unique_vs_encounters.update(vital_signs[\"encounter_id\"].unique())\n",
    "\n",
    "print(\"Vitals\", len(unique_vs_encounters))\n",
    "\n",
    "# This block fo code converts the set to a dataframe because the write command is faster.\n",
    "df_unique_encounters = pd.DataFrame(list(unique_vs_encounters), columns=[\"encounter_id\"])\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"vitals_encounters.csv\")\n",
    "df_unique_encounters.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Unique encounter IDs with vital signs reported are written to {output_file}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vital Signs to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: vital signs to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "unique_vs_encounters_list = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"vitals_encounters.csv\"))[\"encounter_id\"])\n",
    "print(\"Vitals\", len(unique_vs_encounters_list))\n",
    "# Should be Vitals 101036030"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnoses to CSV - errors for memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legacy code, runs in to memory issues. \n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "num_spreadsheets = 100\n",
    "#num_spreadsheets = 1273\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "unique_diag_encounters = set()\n",
    "\n",
    "for i in range(1, num_spreadsheets + 1):\n",
    "    print(f'{i:04}')  \n",
    "    diagnoses = pd.read_csv(\n",
    "        database_dir + r\"Diagnosis\\diagnosis\"+f'{i:04}'+\".csv\",\n",
    "        names=columns,          # Override column names\n",
    "        usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "        dtype={\"encounter_id\": str}  # Ensure \"encounter_id\" is read as a string\n",
    "    )\n",
    "    unique_diag_encounters.update(diagnoses[\"encounter_id\"].unique())\n",
    "\n",
    "print(\"Diagnoses \", len(unique_diag_encounters))\n",
    "\n",
    "# This block fo code converts the set to a dataframe because the write command is faster.\n",
    "df_unique_encounters = pd.DataFrame(list(unique_diag_encounters), columns=[\"encounter_id\"])\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"diagnosis_encounters.csv\")\n",
    "df_unique_encounters.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Unique encounter IDs with diagnoses reported are written to {output_file}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the storage file (HDF5)\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "\n",
    "num_spreadsheets = 1273\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"diagnosis_encounters.csv\")\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Diagnosis/diagnosis{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str}  # Ensure \"encounter_id\" is read as a string\n",
    "        )\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "# Read data using Dask\n",
    "dask_df = dd.read_hdf(store_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "dask_df = dask_df.drop_duplicates()\n",
    "\n",
    "# Compute and save to CSV\n",
    "dask_df.compute().to_csv(output_file, index=False)\n",
    "    \n",
    "print(f\"Unique encounter IDs with diagnoses reported are written to {output_file}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish off conversion to dask\n",
    "\n",
    "# Retrieve unique encounter IDs\n",
    "try: \n",
    "    with pd.HDFStore(store_path) as store:\n",
    "        # Initialize an empty DataFrame for deduplicated data\n",
    "        deduplicated_encounters = pd.DataFrame()\n",
    "\n",
    "        # Process in chunks\n",
    "        chunk_size = 500000  # Adjust chunk size based on your system's memory\n",
    "        iterator = store.select('unique_encounters', chunksize=chunk_size)\n",
    "        for chunk in iterator:\n",
    "            # Drop duplicates within each chunk\n",
    "            chunk = chunk.drop_duplicates()\n",
    "            # Append deduplicated chunk to file\n",
    "            chunk.to_csv(output_file, mode='a', index=False, header=not bool(deduplicated_encounters.size))\n",
    "\n",
    "        # Optionally, read back the full file to remove duplicates that might span chunks\n",
    "        deduplicated_full = pd.read_csv(output_file).drop_duplicates()\n",
    "        deduplicated_full.to_csv(output_file, index=False)\n",
    "finally: \n",
    "    store.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "unique_diag_encounters_list = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"diagnosis_encounters.csv\"))[\"encounter_id\"])\n",
    "print(\"Diagnosis\", len(unique_diag_encounters_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2334\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"date\",\"value\",\"text_value\",\"units_of_measure\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "unique_lab_encounters = set()\n",
    "\n",
    "for i in range(1, num_spreadsheets + 1):\n",
    "    print(f'{i:04}')  \n",
    "    labs = pd.read_csv(\n",
    "        database_dir + r\"Lab Results\\lab_results\"+f'{i:04}'+\".csv\",\n",
    "        names=columns,          # Override column names\n",
    "        usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "        dtype={\"encounter_id\": str}  # Ensure \"encounter_id\" is read as a string\n",
    "    )\n",
    "    unique_lab_encounters.update(labs[\"encounter_id\"].unique())\n",
    "\n",
    "print(\"Lab\", len(unique_lab_encounters))\n",
    "\n",
    "# This block fo code converts the set to a dataframe because the write command is faster.\n",
    "df_unique_encounters = pd.DataFrame(list(unique_lab_encounters), columns=[\"encounter_id\"])\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"labs_encounters.csv\")\n",
    "df_unique_encounters.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Unique encounter IDs with diagnoses reported are written to {output_file}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "unique_lab_encounters_list = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"labs_encounters.csv\"))[\"encounter_id\"])\n",
    "print(\"Diagnosis\", len(unique_lab_encounters_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 714\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_procedure_indicator\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "unique_proc_encounters = set()\n",
    "\n",
    "for i in range(1, num_spreadsheets + 1):\n",
    "    print(f'{i:04}')  \n",
    "    labs = pd.read_csv(\n",
    "        database_dir + r\"Procedure\\procedure\"+f'{i:04}'+\".csv\",\n",
    "        names=columns,          # Override column names\n",
    "        usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "        dtype={\"encounter_id\": str}  # Ensure \"encounter_id\" is read as a string\n",
    "    )\n",
    "    unique_proc_encounters.update(labs[\"encounter_id\"].unique())\n",
    "\n",
    "print(\"Procedure\", len(unique_proc_encounters))\n",
    "\n",
    "df_unique_encounters = pd.DataFrame(list(unique_proc_encounters), columns=[\"encounter_id\"])\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"procedure_encounters.csv\")\n",
    "df_unique_encounters.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Unique encounter IDs with diagnoses reported are written to {output_file}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "unique_proc_encounters_list = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"procedure_encounters.csv\"))[\"encounter_id\"])\n",
    "print(\"Diagnosis\", len(unique_proc_encounters_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Inpatient Medication\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2991\n",
    "columns = [\"patient_id\",\"encounter_id\",\"unique_id\",\"code_system\",\"code\",\"start_date\",\"route\",\"brand\",\"strength\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "unique_med_encounters = set()\n",
    "\n",
    "for i in range(1, num_spreadsheets + 1):\n",
    "    print(f'{i:04}')  \n",
    "    labs = pd.read_csv(\n",
    "        database_dir + r\"Medications\\medication\"+f'{i:04}'+\".csv\",\n",
    "        names=columns,          # Override column names\n",
    "        usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "        dtype={\"encounter_id\": str}  # Ensure \"encounter_id\" is read as a string\n",
    "    )\n",
    "    unique_med_encounters.update(labs[\"encounter_id\"].unique())\n",
    "print(\"Medications\", len(unique_med_encounters))\n",
    "\n",
    "df_unique_encounters = pd.DataFrame(list(unique_med_encounters), columns=[\"encounter_id\"])\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"medication_encounters.csv\")\n",
    "df_unique_encounters.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Unique encounter IDs with diagnoses reported are written to {output_file}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "unique_med_encounters_list = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"medication_encounters.csv\"))[\"encounter_id\"])\n",
    "print(\"Diagnosis\", len(unique_med_encounters_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Ambulatory and Inpatient/ED Screens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Vital Signs\n",
    "unique_vs_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"vitals_encounters.csv\"))[\"encounter_id\"])\n",
    "print(\"Vitals\", len(unique_vs_encounters))\n",
    "\n",
    "# Current Diagnosis\n",
    "unique_diag_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"diagnosis_encounters.csv\"))[\"encounter_id\"])\n",
    "print(\"Diagnosis \", len(unique_diag_encounters))\n",
    "\n",
    "# Print the number of unique patients who passed the data quality check\n",
    "\n",
    "# Filter out patients who did not pass the data quality check\n",
    "new_OP_data_quality_check_FINAL_patients = list(set(unique_vs_encounters) & set(unique_diag_encounters)) # Note, these are all encounter types - but that should be OK\n",
    "print(\"Number of Unique Encounters in OP Filter:\", len(new_OP_data_quality_check_FINAL_patients))\n",
    "\n",
    "df_unique_encounters = pd.DataFrame(list(new_OP_data_quality_check_FINAL_patients), columns=[\"encounter_id\"])\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"amb_enc_screen.csv\")\n",
    "df_unique_encounters.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Unique encounter IDs for Ambulatory Screen are written to {output_file}\")\n",
    "\n",
    "unique_lab_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"labs_encounters.csv\"))[\"encounter_id\"])\n",
    "print(\"Lab\", len(unique_lab_encounters))\n",
    "\n",
    "unique_proc_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"procedure_encounters.csv\"))[\"encounter_id\"])\n",
    "print(\"Procedure \", len(unique_proc_encounters))\n",
    "\n",
    "unique_med_encounters = list(pd.read_csv(os.path.join(working_dir[:-1], \"data_checks\", \"medication_encounters.csv\"))[\"encounter_id\"])\n",
    "print(\"Med \", len(unique_med_encounters))\n",
    "\n",
    "new_IP_data_quality_check_FINAL_patients = list(set(unique_vs_encounters) & set(unique_diag_encounters) & set(unique_med_encounters) & set(unique_proc_encounters) & set(unique_lab_encounters))\n",
    "print(\"Number of Unique Encounters in IP Filter:\", len(new_IP_data_quality_check_FINAL_patients))\n",
    "\n",
    "df_unique_encounters = pd.DataFrame(list(new_IP_data_quality_check_FINAL_patients), columns=[\"encounter_id\"])\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"inp_enc_screen.csv\")\n",
    "df_unique_encounters.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Unique encounter IDs for the ED and INP Screen are written to {output_file}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
