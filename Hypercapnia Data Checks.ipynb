{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Data Check Documents for the Proper Data Control Insurance Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: output files will be placed in the working dir\n",
    "\n",
    "#PC: \n",
    "database_dir = r\"E:\\TriNetX\\\\\"   # Location where the database files are stored \n",
    "working_dir = r\"C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\\\\" #location where to read and right from (faster = better if space allows)\n",
    "\n",
    "#Mac \n",
    "#database_dir = r\"/Volumes/LOCKE STUDY/TriNetX\"   # Location where the database files are stored \n",
    "#working_dir = r\"/Users/blocke/TriNetX Working/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import logging\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import config\n",
    "import h5py\n",
    "\n",
    "#Create an output directory if it's not already there\n",
    "os.makedirs(os.path.join(working_dir[:-1], \"data_checks\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make HD5 Files with each type of data element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vital Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vital Signs\n",
    "start_time = time.time()\n",
    "store_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 853\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Vital Signs/vital_signs{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diagnoses \n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "num_spreadsheets = 1273\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Diagnosis/diagnosis{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2334\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"date\",\"value\",\"text_value\",\"units_of_measure\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Lab Results/lab_results{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedures\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 714\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_procedure_indicator\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Procedure/procedure{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2991\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"unique_id\",\"code_system\",\"code\",\"start_date\",\"route\",\"brand\",\"strength\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Medications/medication{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge All Encounters of Ambulatory and Emerg/Inp, then deduplicate and make screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H5 Structure Check Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of diag_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 249836036 entries\n",
      "Structure of vitals_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 101603643 entries\n",
      "Structure of lab_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 100800073 entries\n",
      "Structure of med_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 159229970 entries\n",
      "Structure of proc_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 166304142 entries\n"
     ]
    }
   ],
   "source": [
    "def print_structure_and_count(name, obj):\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(f\"{name}: {len(obj)} entries\")\n",
    "    else:\n",
    "        print(name)\n",
    "\n",
    "def check_and_print_structure(file_path):\n",
    "    print(f\"Structure of {os.path.basename(file_path)}:\")\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        f.visititems(print_structure_and_count)\n",
    "\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "lab_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "med_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "proc_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# List of all HDF5 paths\n",
    "paths = [diag_path, vitals_path, lab_path, med_path, proc_path]\n",
    "\n",
    "for path in paths:\n",
    "    check_and_print_structure(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge All Ambulatory Screen Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read H5 files into Dask DataFrames.\n",
      "Successfully converted 'encounter_id' to strings and trimmed whitespace.\n",
      "Successfully merged DataFrames.\n",
      "Successfully persisted merged DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:13:07,325 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8bde658154dad93add289e311b1f670 initialized by task ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 21) executed on worker tcp://127.0.0.1:51962\n",
      "2024-05-27 19:17:43,211 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a887f5b712a137915e21f972dc6f1ace initialized by task ('hash-join-transfer-a887f5b712a137915e21f972dc6f1ace', 99) executed on worker tcp://127.0.0.1:51967\n",
      "2024-05-27 19:22:27,115 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8bde658154dad93add289e311b1f670 deactivated due to stimulus 'task-finished-1716859347.1079235'\n",
      "2024-05-27 19:22:27,240 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a887f5b712a137915e21f972dc6f1ace deactivated due to stimulus 'task-finished-1716859347.1079235'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged unique encounters saved to: C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\merged_unique_encounters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:23:50,505 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-27 19:23:50,538 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-27 19:23:50,540 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-27 19:23:50,541 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "output_path = os.path.join(working_dir[:-1], 'amb_screen_all_encounters.csv')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6GB'\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,               # Number of worker processes\n",
    "    threads_per_worker=1,      # Number of threads per worker\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.70,\n",
    "    'distributed.worker.memory.spill': 0.80,\n",
    "    'distributed.worker.memory.pause': 0.80,\n",
    "    'distributed.worker.memory.terminate': 0.95,\n",
    "    'distributed.scheduler.allowed-failures': 10,\n",
    "})\n",
    "\n",
    "# Read the data using Dask\n",
    "try:\n",
    "    diag_ddf = dd.read_hdf(diag_path, 'unique_encounters/table')\n",
    "    vitals_ddf = dd.read_hdf(vitals_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    diag_ddf = diag_ddf[['encounter_id']]\n",
    "    vitals_ddf = vitals_ddf[['encounter_id']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    diag_ddf['encounter_id'] = diag_ddf['encounter_id'].astype(str).str.strip()\n",
    "    vitals_ddf['encounter_id'] = vitals_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing 'encounter_id': {e}\")\n",
    "\n",
    "\n",
    "# Perform the merge operation to keep only common \"encounter_id\"\n",
    "try:\n",
    "    merged_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "    print(\"Successfully merged DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging DataFrames: {e}\")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "try:\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "    print(\"Successfully persisted merged DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error persisting DataFrame: {e}\")\n",
    "\n",
    "# Write the result to a single CSV file\n",
    "try:\n",
    "    merged_ddf.to_csv(output_path, single_file=True, index=False)\n",
    "    print(f\"Merged unique encounters saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Resulting Size to Ensure It's Plausible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: (64220008, 1)\n"
     ]
    }
   ],
   "source": [
    "output_csv_path = os.path.join(working_dir[:-1], 'amb_screen_all_encounters.csv')\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplicate and make screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed in 0 hours, 2 minutes, and 32.43 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small enough to just use pandas\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "input_csv_path  = os.path.join(working_dir[:-1], 'amb_screen_all_encounters.csv')\n",
    "output_csv_path = os.path.join(working_dir[:-1], \"data_checks\", \"amb_enc_screen.csv\")\n",
    "\n",
    "\n",
    "# Read the data using Pandas\n",
    "pdf = pd.read_csv(input_csv_path, \n",
    "                  usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "                  dtype={\"encounter_id\": str})\n",
    "\n",
    "# Remove duplicates\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pdf.to_csv(output_csv_path, index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: (63336233, 1)\n"
     ]
    }
   ],
   "source": [
    "output_csv_path = os.path.join(working_dir[:-1], \"data_checks\", \"amb_enc_screen.csv\")\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge All Inpatient and Emergency Screen Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 54124 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read diag+vitals H5 files into Dask DataFrames.\n",
      "Successfully converted diag+vitals 'encounter_id' to strings and trimmed whitespace.\n",
      "Successfully merged diag+vitals DataFrames.\n",
      "Successfully persisted merged DataFrame.\n",
      "Successfully read lab H5 files into Dask DataFrames.\n",
      "Successfully converted lab 'encounter_id' to strings and trimmed whitespace.\n",
      "Successfully merged lab DataFrames.\n",
      "Successfully persisted merged DataFrame.\n",
      "Successfully read proc H5 files into Dask DataFrames.\n",
      "Successfully converted proc 'encounter_id' to strings and trimmed whitespace.\n",
      "Successfully merged proc DataFrames.\n",
      "Successfully persisted merged DataFrame.\n",
      "Successfully read med H5 files into Dask DataFrames.\n",
      "Successfully converted med 'encounter_id' to strings and trimmed whitespace.\n",
      "Successfully merged med DataFrames.\n",
      "Successfully persisted merged DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 06:34:28,953 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8bde658154dad93add289e311b1f670 initialized by task ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 183) executed on worker tcp://127.0.0.1:54154\n",
      "2024-05-28 06:34:34,540 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 5a9e2e5c503ac8330c6600ca3bdf80de initialized by task ('hash-join-transfer-5a9e2e5c503ac8330c6600ca3bdf80de', 146) executed on worker tcp://127.0.0.1:54154\n",
      "2024-05-28 06:34:35,986 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 81dd9ab57fa23a78b528790d5ba143ae initialized by task ('hash-join-transfer-81dd9ab57fa23a78b528790d5ba143ae', 156) executed on worker tcp://127.0.0.1:54147\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-13031626' coro=<Client._gather.<locals>.wait() done, defined at c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\client.py:2197> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\client.py\", line 2206, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 179\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Write the result to a single CSV file\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     merged_ddf\u001b[38;5;241m.\u001b[39mto_csv(output_path, single_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerged unique encounters saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask_expr\\_collection.py:2425\u001b[0m, in \u001b[0;36mFrameBase.to_csv\u001b[1;34m(self, filename, **kwargs)\u001b[0m\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See dd.to_csv docstring for more information\"\"\"\u001b[39;00m\n\u001b[0;32m   2423\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_expr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_csv\n\u001b[1;32m-> 2425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_csv(\u001b[38;5;28mself\u001b[39m, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask_expr\\io\\csv.py:274\u001b[0m, in \u001b[0;36mto_csv\u001b[1;34m(df, filename, single_file, encoding, mode, name_function, compression, compute, scheduler, storage_options, header_first_partition_only, compute_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03mStore Dask DataFrame to CSV files\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03mfsspec.open_files\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_csv \u001b[38;5;28;01mas\u001b[39;00m _to_csv\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _to_csv(\n\u001b[0;32m    275\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_legacy_dataframe(),\n\u001b[0;32m    276\u001b[0m     filename,\n\u001b[0;32m    277\u001b[0m     single_file\u001b[38;5;241m=\u001b[39msingle_file,\n\u001b[0;32m    278\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    279\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    280\u001b[0m     name_function\u001b[38;5;241m=\u001b[39mname_function,\n\u001b[0;32m    281\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    282\u001b[0m     compute\u001b[38;5;241m=\u001b[39mcompute,\n\u001b[0;32m    283\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[0;32m    284\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    285\u001b[0m     header_first_partition_only\u001b[38;5;241m=\u001b[39mheader_first_partition_only,\n\u001b[0;32m    286\u001b[0m     compute_kwargs\u001b[38;5;241m=\u001b[39mcompute_kwargs,\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    288\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:1005\u001b[0m, in \u001b[0;36mto_csv\u001b[1;34m(df, filename, single_file, encoding, mode, name_function, compression, compute, scheduler, storage_options, header_first_partition_only, compute_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1001\u001b[0m         compute_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m scheduler\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(dask\u001b[38;5;241m.\u001b[39mcompute(\u001b[38;5;241m*\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompute_kwargs))\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\dask\\base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\reblo\\anaconda3\\Lib\\threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "lab_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "proc_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "med_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "output_path = os.path.join(working_dir[:-1], 'inp_screen_all_encounters.csv')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6GB'\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,               # Number of worker processes\n",
    "    threads_per_worker=1,      # Number of threads per worker\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.70,\n",
    "    'distributed.worker.memory.spill': 0.80,\n",
    "    'distributed.worker.memory.pause': 0.80,\n",
    "    'distributed.worker.memory.terminate': 0.95,\n",
    "    'distributed.scheduler.allowed-failures': 10,\n",
    "})\n",
    "\n",
    "# Read the data using Dask\n",
    "try:\n",
    "    diag_ddf = dd.read_hdf(diag_path, 'unique_encounters/table')\n",
    "    vitals_ddf = dd.read_hdf(vitals_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read diag+vitals H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 diag+vitals files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    diag_ddf = diag_ddf[['encounter_id']]\n",
    "    vitals_ddf = vitals_ddf[['encounter_id']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting diag+vitals columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    diag_ddf['encounter_id'] = diag_ddf['encounter_id'].astype(str).str.strip()\n",
    "    vitals_ddf['encounter_id'] = vitals_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted diag+vitals 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing diag+vitals 'encounter_id': {e}\")\n",
    "\n",
    "\n",
    "# Perform the merge operation to keep only common \"encounter_id\"\n",
    "try:\n",
    "    merged_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "    del diag_ddf\n",
    "    del vitals_ddf\n",
    "    print(\"Successfully merged diag+vitals DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging diag+vitals DataFrames: {e}\")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "try:\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "    print(\"Successfully persisted merged DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error persisting DataFrame: {e}\")\n",
    "\n",
    "#-----------------------------\n",
    "# Now add labs\n",
    "try:\n",
    "    lab_ddf = dd.read_hdf(lab_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read lab H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 lab files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    lab_ddf = lab_ddf[['encounter_id']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting lab columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    lab_ddf['encounter_id'] = lab_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted lab 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing lab 'encounter_id': {e}\")\n",
    "\n",
    "# Perform the merge operation to keep only common \"encounter_id\"\n",
    "try:\n",
    "    merged_ddf = merged_ddf.merge(lab_ddf, on='encounter_id', how='inner')\n",
    "    del lab_ddf\n",
    "    print(\"Successfully merged lab DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging lab DataFrames: {e}\")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "try:\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "    print(\"Successfully persisted merged DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error persisting DataFrame: {e}\")\n",
    "\n",
    "#-----------------------------\n",
    "# Now add procedures\n",
    "try:\n",
    "    proc_ddf = dd.read_hdf(proc_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read proc H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 proc files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    proc_ddf = proc_ddf[['encounter_id']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting proc columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    proc_ddf['encounter_id'] = proc_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted proc 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing proc 'encounter_id': {e}\")\n",
    "\n",
    "# Perform the merge operation to keep only common \"encounter_id\"\n",
    "try:\n",
    "    merged_ddf = merged_ddf.merge(proc_ddf, on='encounter_id', how='inner')\n",
    "    del proc_ddf\n",
    "    print(\"Successfully merged proc DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging proc DataFrames: {e}\")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "try:\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "    print(\"Successfully persisted merged DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error persisting DataFrame: {e}\")\n",
    "\n",
    "#-----------------------------\n",
    "# Now add meds\n",
    "try:\n",
    "    med_ddf = dd.read_hdf(med_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read med H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 med files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    med_ddf = med_ddf[['encounter_id']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting med columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    med_ddf['encounter_id'] = med_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted med 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing med 'encounter_id': {e}\")\n",
    "\n",
    "# Perform the merge operation to keep only common \"encounter_id\"\n",
    "try:\n",
    "    merged_ddf = merged_ddf.merge(med_ddf, on='encounter_id', how='inner')\n",
    "    del med_ddf\n",
    "    print(\"Successfully merged med DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging med DataFrames: {e}\")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "try:\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "    print(\"Successfully persisted merged DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error persisting DataFrame: {e}\")\n",
    "\n",
    "# Write the result to a single CSV file\n",
    "try:\n",
    "    merged_ddf.to_csv(output_path, single_file=True, index=False)\n",
    "    print(f\"Merged unique encounters saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to see if plausible size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: (16419594, 1)\n"
     ]
    }
   ],
   "source": [
    "output_csv_path = os.path.join(working_dir[:-1], 'inp_screen_all_encounters.csv')\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplicate and make screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed in 0 hours, 0 minutes, and 15.13 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small enough to just use pandas\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "input_csv_path  = os.path.join(working_dir[:-1], 'inp_screen_all_encounters.csv')\n",
    "output_csv_path = os.path.join(working_dir[:-1], \"data_checks\", \"inp_enc_screen.csv\")\n",
    "\n",
    "\n",
    "# Read the data using Pandas\n",
    "pdf = pd.read_csv(input_csv_path, \n",
    "                  usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "                  dtype={\"encounter_id\": str})\n",
    "\n",
    "# Remove duplicates\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pdf.to_csv(output_csv_path, index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: (12564033, 1)\n"
     ]
    }
   ],
   "source": [
    "output_csv_path = os.path.join(working_dir[:-1], \"data_checks\", \"inp_enc_screen.csv\")\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
