{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Data Check Documents for the Proper Data Control Insurance Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: output files will be placed in the working dir\n",
    "\n",
    "#PC: \n",
    "database_dir = r\"E:\\TriNetX\\\\\"   # Location where the database files are stored \n",
    "working_dir = r\"C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\\\\" #location where to read and right from (faster = better if space allows)\n",
    "\n",
    "#Mac \n",
    "#database_dir = r\"/Volumes/LOCKE STUDY/TriNetX\"   # Location where the database files are stored \n",
    "#working_dir = r\"/Users/blocke/TriNetX Working/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import logging\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import config\n",
    "import h5py\n",
    "\n",
    "#Create an output directory if it's not already there\n",
    "os.makedirs(os.path.join(working_dir[:-1], \"data_checks\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make HD5 Files with each type of data element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vital Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vital Signs\n",
    "start_time = time.time()\n",
    "store_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 853\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Vital Signs/vital_signs{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diagnoses \n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "num_spreadsheets = 1273\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Diagnosis/diagnosis{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2334\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"date\",\"value\",\"text_value\",\"units_of_measure\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Lab Results/lab_results{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedures\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 714\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_procedure_indicator\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Procedure/procedure{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "#num_spreadsheets = 10\n",
    "num_spreadsheets = 2991\n",
    "\n",
    "columns = [\"patient_id\",\"encounter_id\",\"unique_id\",\"code_system\",\"code\",\"start_date\",\"route\",\"brand\",\"strength\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Medications/medication{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str},  # Ensure \"encounter_id\" is read as a string\n",
    "            skiprows=1 if i == 1 else 0   # Skip the first row only for the first file\n",
    "        )\n",
    "        chunk.drop_duplicates(subset=[\"encounter_id\"], inplace=True)\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H5 Structure Check Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of diag_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 249836036 entries\n",
      "Structure of vitals_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 101603643 entries\n",
      "Structure of lab_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 100800073 entries\n",
      "Structure of med_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 159229970 entries\n",
      "Structure of proc_unique_encounters.h5:\n",
      "unique_encounters\n",
      "unique_encounters/table: 166304142 entries\n"
     ]
    }
   ],
   "source": [
    "def print_structure_and_count(name, obj):\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(f\"{name}: {len(obj)} entries\")\n",
    "    else:\n",
    "        print(name)\n",
    "\n",
    "def check_and_print_structure(file_path):\n",
    "    print(f\"Structure of {os.path.basename(file_path)}:\")\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        f.visititems(print_structure_and_count)\n",
    "\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "lab_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "med_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "proc_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# List of all HDF5 paths\n",
    "paths = [diag_path, vitals_path, lab_path, med_path, proc_path]\n",
    "\n",
    "for path in paths:\n",
    "    check_and_print_structure(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Attempt: try to merge the databases first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambulatory Screen Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read H5 files into Dask DataFrames.\n",
      "Successfully converted 'encounter_id' to strings and trimmed whitespace.\n",
      "Successfully merged DataFrames.\n",
      "Successfully persisted merged DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:13:07,325 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8bde658154dad93add289e311b1f670 initialized by task ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 21) executed on worker tcp://127.0.0.1:51962\n",
      "2024-05-27 19:17:43,211 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a887f5b712a137915e21f972dc6f1ace initialized by task ('hash-join-transfer-a887f5b712a137915e21f972dc6f1ace', 99) executed on worker tcp://127.0.0.1:51967\n",
      "2024-05-27 19:22:27,115 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8bde658154dad93add289e311b1f670 deactivated due to stimulus 'task-finished-1716859347.1079235'\n",
      "2024-05-27 19:22:27,240 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a887f5b712a137915e21f972dc6f1ace deactivated due to stimulus 'task-finished-1716859347.1079235'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged unique encounters saved to: C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\merged_unique_encounters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:23:50,505 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-27 19:23:50,538 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-27 19:23:50,540 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-27 19:23:50,541 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "output_path = os.path.join(working_dir[:-1], 'amb_screen_unique_encounters.csv')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6GB'\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,               # Number of worker processes\n",
    "    threads_per_worker=1,      # Number of threads per worker\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.70,\n",
    "    'distributed.worker.memory.spill': 0.80,\n",
    "    'distributed.worker.memory.pause': 0.80,\n",
    "    'distributed.worker.memory.terminate': 0.95,\n",
    "    'distributed.scheduler.allowed-failures': 10,\n",
    "})\n",
    "\n",
    "# Read the data using Dask\n",
    "try:\n",
    "    diag_ddf = dd.read_hdf(diag_path, 'unique_encounters/table')\n",
    "    vitals_ddf = dd.read_hdf(vitals_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    diag_ddf = diag_ddf[['encounter_id']]\n",
    "    vitals_ddf = vitals_ddf[['encounter_id']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    diag_ddf['encounter_id'] = diag_ddf['encounter_id'].astype(str).str.strip()\n",
    "    vitals_ddf['encounter_id'] = vitals_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing 'encounter_id': {e}\")\n",
    "\n",
    "\n",
    "# Perform the merge operation to keep only common \"encounter_id\"\n",
    "try:\n",
    "    merged_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "    print(\"Successfully merged DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging DataFrames: {e}\")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "try:\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "    print(\"Successfully persisted merged DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error persisting DataFrame: {e}\")\n",
    "\n",
    "# Write the result to a single CSV file\n",
    "try:\n",
    "    merged_ddf.to_csv(output_path, single_file=True, index=False)\n",
    "    print(f\"Merged unique encounters saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Resulting Size to Ensure It's Plausible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: (64220008, 1)\n"
     ]
    }
   ],
   "source": [
    "output_csv_path = os.path.join(working_dir[:-1], 'amb_screen_unique_encounters.csv')\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inpatient and Emergency Screen Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reblo\\anaconda3\\Lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 52614 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read diag+vitals H5 files into Dask DataFrames.\n",
      "Successfully converted diag+vitals 'encounter_id' to strings and trimmed whitespace.\n",
      "Successfully merged diag+vitals DataFrames.\n",
      "Successfully persisted merged DataFrame.\n",
      "Successfully read lab H5 files into Dask DataFrames.\n",
      "Successfully converted lab 'encounter_id' to strings and trimmed whitespace.\n",
      "Successfully merged lab DataFrames.\n",
      "Successfully persisted merged DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 21:36:21,338 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8bde658154dad93add289e311b1f670 initialized by task ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 30) executed on worker tcp://127.0.0.1:52645\n",
      "2024-05-27 21:41:12,084 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle e078be2e087dc34d857c8c6c60a31f19 initialized by task ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 98) executed on worker tcp://127.0.0.1:52638\n",
      "2024-05-27 21:43:05,667 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a887f5b712a137915e21f972dc6f1ace initialized by task ('hash-join-transfer-a887f5b712a137915e21f972dc6f1ace', 99) executed on worker tcp://127.0.0.1:52559\n",
      "2024-05-27 21:49:25,342 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8bde658154dad93add289e311b1f670 deactivated due to stimulus 'task-finished-1716868165.3324838'\n",
      "2024-05-27 21:49:25,385 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a887f5b712a137915e21f972dc6f1ace deactivated due to stimulus 'task-finished-1716868165.3324838'\n",
      "2024-05-27 21:50:57,442 - distributed.scheduler - WARNING - Worker failed to heartbeat for 349s; attempting restart: <WorkerState 'tcp://127.0.0.1:52645', name: 3, status: running, memory: 61, processing: 5>\n",
      "2024-05-27 21:51:01,475 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-05-27 21:51:01,564 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:52645' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('hash-join-transfer-a8bde658154dad93add289e311b1f670', 219), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 228), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 173), ('to_pyarrow_string-5262c03411858bb3e824bdac1ae2dc8b', 48), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 237), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 60), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 179), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 124), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 63), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 182), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 246), ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 59), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 197), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 81), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 200), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 84), ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 52), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 148), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 212), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 90), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 154), ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 67), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 230), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 111), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 59), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 245), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 184), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 190), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 123), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 4), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 193), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 138), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 83), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 22), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 196), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 86), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 205), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 31), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 95), ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 60), ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 66), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 165), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 55), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 113), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 52), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 122), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 128), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 247), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 241), ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 90), ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 93), ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 99), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 189), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 15), ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 84), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 140), ('to_pyarrow_string-e58f92d8c86c015b7688b494fe00925d', 201), ('getitem-fused-assign-1eaf92f72243f7d92d1af23c2ac434f0', 49), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 204), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 143), ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 30)} (stimulus_id='handle-worker-cleanup-1716868261.5516317')\n",
      "2024-05-27 21:51:01,587 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8bde658154dad93add289e311b1f670 deactivated due to stimulus 'handle-worker-cleanup-1716868261.5516317'\n",
      "2024-05-27 21:51:01,592 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle e078be2e087dc34d857c8c6c60a31f19 deactivated due to stimulus 'handle-worker-cleanup-1716868261.5516317'\n",
      "2024-05-27 21:51:01,637 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8bde658154dad93add289e311b1f670 restarted due to stimulus 'handle-worker-cleanup-1716868261.5516317\n",
      "2024-05-27 21:51:01,658 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle e078be2e087dc34d857c8c6c60a31f19 restarted due to stimulus 'handle-worker-cleanup-1716868261.5516317\n",
      "2024-05-27 21:51:01,771 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-05-27 21:51:06,578 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8bde658154dad93add289e311b1f670 initialized by task ('hash-join-transfer-a8bde658154dad93add289e311b1f670', 25) executed on worker tcp://127.0.0.1:52637\n",
      "2024-05-27 21:51:06,606 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle e078be2e087dc34d857c8c6c60a31f19 initialized by task ('hash-join-transfer-e078be2e087dc34d857c8c6c60a31f19', 28) executed on worker tcp://127.0.0.1:52638\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "diag_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "lab_path = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "proc_path = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "med_path = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "output_path = os.path.join(working_dir[:-1], 'inp_screen_unique_encounters.csv')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6GB'\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,               # Number of worker processes\n",
    "    threads_per_worker=1,      # Number of threads per worker\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.70,\n",
    "    'distributed.worker.memory.spill': 0.80,\n",
    "    'distributed.worker.memory.pause': 0.80,\n",
    "    'distributed.worker.memory.terminate': 0.95,\n",
    "    'distributed.scheduler.allowed-failures': 10,\n",
    "})\n",
    "\n",
    "# Read the data using Dask\n",
    "try:\n",
    "    diag_ddf = dd.read_hdf(diag_path, 'unique_encounters/table')\n",
    "    vitals_ddf = dd.read_hdf(vitals_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read diag+vitals H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 diag+vitals files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    diag_ddf = diag_ddf[['encounter_id']]\n",
    "    vitals_ddf = vitals_ddf[['encounter_id']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting diag+vitals columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    diag_ddf['encounter_id'] = diag_ddf['encounter_id'].astype(str).str.strip()\n",
    "    vitals_ddf['encounter_id'] = vitals_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted diag+vitals 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing diag+vitals 'encounter_id': {e}\")\n",
    "\n",
    "\n",
    "# Perform the merge operation to keep only common \"encounter_id\"\n",
    "try:\n",
    "    merged_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "    del diag_ddf\n",
    "    del vitals_ddf\n",
    "    print(\"Successfully merged diag+vitals DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging diag+vitals DataFrames: {e}\")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "try:\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "    print(\"Successfully persisted merged DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error persisting DataFrame: {e}\")\n",
    "\n",
    "#-----------------------------\n",
    "# Now add labs\n",
    "try:\n",
    "    lab_ddf = dd.read_hdf(lab_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read lab H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 lab files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    lab_ddf = lab_ddf[['encounter_id']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting lab columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    lab_ddf['encounter_id'] = lab_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted lab 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing lab 'encounter_id': {e}\")\n",
    "\n",
    "# Perform the merge operation to keep only common \"encounter_id\"\n",
    "try:\n",
    "    merged_ddf = merged_ddf.merge(lab_ddf, on='encounter_id', how='inner')\n",
    "    del lab_ddf\n",
    "    print(\"Successfully merged lab DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging lab DataFrames: {e}\")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "try:\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "    print(\"Successfully persisted merged DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error persisting DataFrame: {e}\")\n",
    "\n",
    "#-----------------------------\n",
    "# Now add procedures\n",
    "try:\n",
    "    proc_ddf = dd.read_hdf(proc_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read proc H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 proc files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    proc_ddf = proc_ddf[['encounter_id']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting proc columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    proc_ddf['encounter_id'] = proc_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted proc 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing proc 'encounter_id': {e}\")\n",
    "\n",
    "# Perform the merge operation to keep only common \"encounter_id\"\n",
    "try:\n",
    "    merged_ddf = merged_ddf.merge(proc_ddf, on='encounter_id', how='inner')\n",
    "    del proc_ddf\n",
    "    print(\"Successfully merged proc DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging proc DataFrames: {e}\")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "try:\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "    print(\"Successfully persisted merged DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error persisting DataFrame: {e}\")\n",
    "\n",
    "#-----------------------------\n",
    "# Now add meds\n",
    "try:\n",
    "    med_ddf = dd.read_hdf(med_path, 'unique_encounters/table')\n",
    "    print(\"Successfully read med H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 med files into Dask DataFrames: {e}\")\n",
    "\n",
    "# Drop the index column if it exists\n",
    "try:\n",
    "    med_ddf = med_ddf[['encounter_id']]\n",
    "except Exception as e:\n",
    "    print(f\"Error selecting med columns: {e}\")\n",
    "\n",
    "# Convert 'encounter_id' to string and trim whitespace\n",
    "try:\n",
    "    med_ddf['encounter_id'] = med_ddf['encounter_id'].astype(str).str.strip()\n",
    "    print(\"Successfully converted med 'encounter_id' to strings and trimmed whitespace.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing med 'encounter_id': {e}\")\n",
    "\n",
    "# Perform the merge operation to keep only common \"encounter_id\"\n",
    "try:\n",
    "    merged_ddf = merged_ddf.merge(med_ddf, on='encounter_id', how='inner')\n",
    "    del med_ddf\n",
    "    print(\"Successfully merged med DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging med DataFrames: {e}\")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "try:\n",
    "    merged_ddf = merged_ddf.persist()\n",
    "    print(\"Successfully persisted merged DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error persisting DataFrame: {e}\")\n",
    "\n",
    "# Write the result to a single CSV file\n",
    "try:\n",
    "    merged_ddf.to_csv(output_path, single_file=True, index=False)\n",
    "    print(f\"Merged unique encounters saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to see if plausible size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = os.path.join(working_dir[:-1], 'inp_screen_unique_encounters.csv')\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small enough to just use pandas\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_lab_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Pandas\n",
    "pdf = pd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pdf.to_csv(output_csv_path, index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vital Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vital Signs - small enough to do simply\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'vitals_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds \n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_proc_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meds \n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'med_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Reading data from HDF5\n",
      "INFO:__main__:Dropping duplicates in the full dataset\n",
      "INFO:__main__:Computing the full dataset to remove duplicates\n"
     ]
    }
   ],
   "source": [
    "# Troubleshoot code block\n",
    "# Try this one first? \n",
    "#  WHICH DIAGNOSIS VERSION WORKS? - Allegedly this one does on mac. \n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "logger.info(\"Reading data from HDF5\")\n",
    "# This is a hack to delay the compute\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Sample the data to check if it's a memory issue\n",
    "logger.info(\"Taking a sample of the data for testing\")\n",
    "sample_ddf = ddf.sample(frac=0.1).compute()\n",
    "logger.info(f\"Sample size: {sample_ddf.shape}\")\n",
    "\n",
    "# Check for duplicates in the sample\n",
    "logger.info(\"Dropping duplicates in the sample\")\n",
    "sample_ddf = sample_ddf.drop_duplicates()\n",
    "\n",
    "# Test writing the sample to CSV\n",
    "sample_output_path = os.path.join(working_dir[:-1], 'sample_clean_diag_unique_encounters.csv')\n",
    "logger.info(f\"Writing sample data to {sample_output_path}\")\n",
    "sample_ddf.to_csv(sample_output_path, index=False)\n",
    "\n",
    "# If the sample works, proceed with the full dataset\n",
    "logger.info(\"Dropping duplicates in the full dataset\")\n",
    "ddf = ddf.drop_duplicates()\n",
    "logger.info(\"Computing the full dataset to remove duplicates\")\n",
    "try:\n",
    "    ddf = ddf.compute()\n",
    "    logger.info(\"Full dataset computed successfully\")\n",
    "\n",
    "    # Write the full dataset to CSV\n",
    "    logger.info(f\"Writing full dataset to {output_csv_path}\")\n",
    "    ddf.to_csv(output_csv_path, index=False)\n",
    "except Exception as e:\n",
    "    logger.error(\"Error during compute or writing to CSV\", exc_info=True)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Screens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambulatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\clean_diag_unique_encounters.csv\n",
      "C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\clean_vitals_unique_encounters.csv\n",
      "C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\data_checks\\amb_enc_screen.csv\n",
      "Successfully read CSV files into Dask DataFrames.\n",
      "Successfully merged DataFrames.\n",
      "Intersection CSV saved to: C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\data_checks\\amb_enc_screen.csv\n",
      "Shape of output: (2659269, 1)\n",
      "Executed in 0 hours, 0 minutes, and 15.94 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5311"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Ambulatory\n",
    "# Read data from HDF5 files using Dask\n",
    "diag_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "print(diag_path)\n",
    "vitals_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "print(vitals_path)\n",
    "output_csv_path = os.path.join(working_dir[:-1], \"data_checks\", \"amb_enc_screen.csv\")\n",
    "print(output_csv_path)\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6pc 8mac cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "\n",
    "try:\n",
    "    # This head hack \n",
    "    diag_ddf = dd.read_csv(diag_path, usecols=['encounter_id']).head(100000000000, compute=False)\n",
    "    vitals_ddf = dd.read_csv(vitals_path, usecols=['encounter_id']).head(100000000000, compute=False)\n",
    "    print(\"Successfully read CSV files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV files into Dask DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    intersection_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "    print(\"Successfully merged DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    # intersection_ddf = intersection_ddf.persist()  \n",
    "    # Persist to avoid recomputation if I were to use this for multiple things - but currently don't\n",
    "    intersection_ddf.to_csv(output_csv_path, single_file=True, index=False)\n",
    "    print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inpatient and Emergency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read CSV files into Dask DataFrames.\n",
      "Diag DF shape (Delayed('int-bd0dec00-131d-45e6-ad81-972d72490991'), 1)\n",
      "post vitals DF shape (Delayed('int-0814e485-58bb-4c5a-8052-4629b8c0bdea'), 1)\n",
      "post encounters DF shape (Delayed('int-e078815b-e4fc-42ab-8a55-aee486778549'), 1)\n",
      "post procDF shape (Delayed('int-620a6dac-38e1-4fb6-8994-e1e74f01c0e2'), 1)\n",
      "post lab DF shape (Delayed('int-3a8bd936-bee6-4ffd-90a1-db8e8f995946'), 1)\n",
      "Successfully merged DataFrames.\n",
      "Intersection CSV saved to: C:\\Users\\reblo\\Box\\Residency Personal Files\\Scholarly Work\\Locke Research Projects\\TriNetX Code\\Hypercapnia TriNetX CSV Processing\\Working\\data_checks\\inp_enc_screen.csv\n",
      "Shape of output: (380772, 1)\n",
      "Executed in 0 hours, 0 minutes, and 31.27 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3119"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Read data from HDF5 files using Dask\n",
    "diag_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "vitals_path = os.path.join(working_dir[:-1], 'clean_vitals_unique_encounters.csv')\n",
    "med_path = os.path.join(working_dir[:-1], 'clean_med_unique_encounters.csv')\n",
    "proc_path = os.path.join(working_dir[:-1], 'clean_proc_unique_encounters.csv')\n",
    "lab_path = os.path.join(working_dir[:-1], 'clean_lab_unique_encounters.csv') \n",
    "\n",
    "output_csv_path = os.path.join(working_dir[:-1], \"data_checks\", \"inp_enc_screen.csv\")\n",
    "\n",
    "# Commented out if prior code block still running\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '6.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6pc 8mac cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "# Read the data using Dask and ensure 'encounter_id' is read as a string\n",
    "try:\n",
    "    # This head hack makes it so the reads are queued, rather than bringing everything in initially.\n",
    "    diag_ddf = dd.read_csv(diag_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    vitals_ddf = dd.read_csv(vitals_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    med_ddf = dd.read_csv(med_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    lab_ddf = dd.read_csv(lab_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    proc_ddf = dd.read_csv(proc_path, usecols=['encounter_id'], dtype={'encounter_id': 'str'}).head(100000000000, compute=False).dropna(subset=['encounter_id'])\n",
    "    print(\"Successfully read CSV files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV files into Dask DataFrames: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    ddf_shape = diag_ddf.shape\n",
    "    print(f\"Diag DF shape {ddf_shape}\")\n",
    "\n",
    "    intersection_ddf = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "    ddf_shape = intersection_ddf.shape\n",
    "    print(f\"post vitals DF shape {ddf_shape}\")\n",
    "\n",
    "    intersection_ddf = intersection_ddf.merge(med_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "    ddf_shape = intersection_ddf.shape\n",
    "    print(f\"post encounters DF shape {ddf_shape}\")\n",
    "\n",
    "    intersection_ddf = intersection_ddf.merge(proc_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "    ddf_shape = intersection_ddf.shape\n",
    "    print(f\"post procDF shape {ddf_shape}\")\n",
    "\n",
    "    intersection_ddf = intersection_ddf.merge(lab_ddf, on='encounter_id', how='inner')\n",
    "    intersection_ddf = intersection_ddf.persist()\n",
    "    ddf_shape = intersection_ddf.shape\n",
    "    print(f\"post lab DF shape {ddf_shape}\")\n",
    "    print(\"Successfully merged DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    # intersection_ddf = intersection_ddf.persist()  \n",
    "    # Persist to avoid recomputation if I were to use this for multiple things - but currently don't\n",
    "    intersection_ddf.to_csv(output_csv_path, single_file=True, index=False)\n",
    "    print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Data check\n",
    "try:\n",
    "    output_csv = pd.read_csv(output_csv_path, usecols=['encounter_id'])\n",
    "    print(f\"Shape of output: {output_csv.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading output CSV file: {e}\")\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Scratchpad commands that didn't make the cut for various reseasons. \n",
    "\n",
    "Preserved in case need to pilfer them later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 Creation - deprecated version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEGACY? \n",
    "\n",
    "\"\"\"\n",
    "# Setup the storage file (HDF5)\n",
    "start_time = time.time()\n",
    "\n",
    "store_path = os.path.join(working_dir[:-1], 'unique_encounters.h5')\n",
    "if os.path.exists(store_path):\n",
    "    try:\n",
    "        # Attempt to open and then immediately close the file\n",
    "        store = pd.HDFStore(store_path)\n",
    "        store.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close the file: {e}\")\n",
    "    os.remove(store_path)  # Ensure a fresh start\n",
    "\n",
    "\n",
    "num_spreadsheets = 1273\n",
    "output_file = os.path.join(working_dir[:-1], \"data_checks\", \"diagnosis_encounters.csv\")\n",
    "columns = [\"patient_id\",\"encounter_id\",\"code_system\",\"code\",\"principal_diagnosis_indicator\",\"admitting_diagnosis\",\"reason_for_visit\",\"date\",\"derived_by_TriNetX\",\"source_id\"]\n",
    "\n",
    "try: \n",
    "    store = pd.HDFStore(store_path)\n",
    "    # Process each CSV and store directly to HDF5\n",
    "    for i in range(1, num_spreadsheets + 1):\n",
    "        print(f'{i:04}')  \n",
    "        file_path = f\"{database_dir}Diagnosis/diagnosis{i:04}.csv\"\n",
    "        chunk = pd.read_csv(file_path,\n",
    "            names=columns,          # Override column names\n",
    "            usecols=[\"encounter_id\"],  # Only read the \"encounter_id\" column\n",
    "            dtype={\"encounter_id\": str}  # Ensure \"encounter_id\" is read as a string\n",
    "        )\n",
    "        store.append('unique_encounters', chunk, format='table', data_columns=True, index=False, min_itemsize={'encounter_id': 12})\n",
    "finally: \n",
    "    store.close()\n",
    "\n",
    "# Read data using Dask\n",
    "dask_df = dd.read_hdf(store_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "dask_df = dask_df.drop_duplicates()\n",
    "\n",
    "# Compute and save to CSV\n",
    "dask_df.compute().to_csv(output_file, index=False)\n",
    "    \n",
    "print(f\"Unique encounter IDs with diagnoses reported are written to {output_file}\")\n",
    "\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version of Procedure Deduplications That Doesn't add Anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAYBE REMOVE:? Currently seems unsuccesful\n",
    "\"\"\"\n",
    "# Proc\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_proc_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'proc_unique_encounters.h5')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '7.50GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,               # Number of worker processes (matching your 6 cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Dask client created with dashboard at: {client.dashboard_link}\")\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.70,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.80,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute()\n",
    "ddf.to_csv(output_csv_path, index = False)\n",
    "\n",
    "# Read the data using Dask\n",
    "try: \n",
    "    ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "    print(\"Successfully read H5 files into Dask DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading H5 files into Dask DataFrames: {e}\")\n",
    "\n",
    "try:\n",
    "    ddf = ddf.drop_duplicates().persist() # Remove duplicates\n",
    "    print(\"Successfully Dropped Duplicates\")\n",
    "except Exception as e:\n",
    "    print(f\"Error Dropping Duplicates: {e}\")\n",
    "\n",
    "\n",
    "try: \n",
    "    ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "    print(\"Successfully Compuated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error Computing: {e}\")\n",
    "\n",
    "try:\n",
    "    # Single-file replaces the compute stage and so it gets directly written to csv\n",
    "    ddf.to_csv(output_csv_path, single_file = True, index = False) \n",
    "    print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "    \n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs - due to size, requires partitioning then recombination.\n",
    "# NOT NEEDED BECAUSE THE REGULAR ONE WORKS FINE\n",
    "\"\"\"\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_lab_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'lab_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "#ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "#ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "\n",
    "num_partitions = 10\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "computed_result = result.compute()\n",
    "\n",
    "# Combine the partitions into a single Pandas DataFrame\n",
    "combined_df = pd.concat(computed_result)\n",
    "\n",
    "# Remove duplicates again to ensure global uniqueness - this is smaller. \n",
    "final_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file\n",
    "final_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "#hdf.to_csv(output_hdf_path, index = False)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Since compute is called, we now work with a Pandas DataFrame\n",
    "# Check and convert data type explicitly if needed\n",
    "if ddf['encounter_id'].dtype != 'object':\n",
    "    ddf['encounter_id'] = ddf['encounter_id'].astype('object')\n",
    "\n",
    "# Now, write the cleaned data frame back to HDF5\n",
    "with pd.HDFStore(output_hdf_path, 'w') as store:\n",
    "    store.put('unique_encounters', ddf, format='table', data_columns=True, index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versions of the Diagnosis De-duplication that didnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one doesn't work because the database is too big. \n",
    "\"\"\"start_time = time.time()\n",
    "\n",
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Pandas\n",
    "pdf = pd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Remove duplicates\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pdf.to_csv(output_csv_path, index=False)\n",
    "\n",
    "#start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = execution_time % 60\n",
    "print(f\"Executed in {hours} hours, {minutes} minutes, and {seconds:.2f} seconds.\")\n",
    "gc.collect()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the working directory and HDF5 paths (update with your actual paths)\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Configure Dask LocalCluster\n",
    "memory_per_worker = '4.00GB'  # Adjust to fit within total memory (7.75GB / 6 workers = ~1.29GB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=2,               # Number of worker processes (matching your 6 cores)\n",
    "    threads_per_worker=1,      # Number of threads per worker to avoid GIL issues\n",
    "    memory_limit=memory_per_worker,  # Memory limit per worker\n",
    "    processes=True,            # Use separate processes for each worker\n",
    "    dashboard_address=':8787'  # Dashboard address for monitoring\n",
    ")\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client(cluster)\n",
    "logger.info(f\"Dask client created with dashboard at: {client.dashboard_link}\")\n",
    "\n",
    "# Adjust memory spilling settings\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.60,    # Spill to disk at 60% memory usage\n",
    "    'distributed.worker.memory.spill': 0.70,     # Spill to disk at 70% memory usage\n",
    "    'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "    'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "    'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10\n",
    "})\n",
    "\n",
    "# Read the data using Dask\n",
    "logger.info(\"Reading data from HDF5\")\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Split the dataframe into 10 smaller chunks\n",
    "num_partitions = 10\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "logger.info(f\"Repartitioned dataframe into {num_partitions} partitions\")\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "logger.info(\"Dropping duplicates in each partition\")\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "logger.info(\"Computing the results for each partition\")\n",
    "computed_result = result.compute()\n",
    "\n",
    "# Combine results and drop duplicates again to ensure global uniqueness\n",
    "logger.info(\"Dropping duplicates from the combined result\")\n",
    "final_result = computed_result.drop_duplicates()\n",
    "\n",
    "# Write the final result to CSV\n",
    "logger.info(f\"Writing final dataset to {output_hdf_path}\")\n",
    "final_result.to_csv(output_hdf_path, index=False)\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "# Diagnoses\n",
    "\n",
    "# Configure Dask\n",
    "cluster = LocalCluster(\n",
    "    n_workers=6,              # Number of workers\n",
    "    threads_per_worker=1,     # Number of threads per worker\n",
    "    memory_limit='1.25GB',       # Memory limit per worker\n",
    "    processes=True,           # Use separate processes for each worker\n",
    "    dashboard_address=':8787' # Dashboard address http://localhost:8787\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "\n",
    "config.set({'distributed.worker.memory.target': 0.33,    # Spill to disk at 60% memory usage\n",
    "            'distributed.worker.memory.spill': 0.40,     # Spill to disk at 70% memory usage\n",
    "            'distributed.worker.memory.pause': 0.70,     # Pause worker at 80% memory usage\n",
    "            'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "            'distributed.scheduler.allowed-failures': 10, # Set the allowed failures to 10          \n",
    "           })\n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_hdf_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "ddf = ddf.drop_duplicates() # Remove duplicates\n",
    "ddf = ddf.compute() # Compute to remove duplicates effectively\n",
    "ddf.to_csv(output_hdf_path, index = False)\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Since compute is called, we now work with a Pandas DataFrame\n",
    "# Check and convert data type explicitly if needed\n",
    "\n",
    "\n",
    "if ddf['encounter_id'].dtype != 'object':\n",
    "    ddf['encounter_id'] = ddf['encounter_id'].astype('object')\n",
    "\n",
    "# Now, write the cleaned data frame back to HDF5\n",
    "with pd.HDFStore(output_hdf_path, 'w') as store:\n",
    "    store.put('unique_encounters', ddf, format='table', data_columns=True, index=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# TODO: WHICH DIAGNOSIS VERSION WORKS? \n",
    "\n",
    "# Define the HDF5 paths\n",
    "output_csv_path = os.path.join(working_dir[:-1], 'clean_diag_unique_encounters.csv')\n",
    "input_hdf_path  = os.path.join(working_dir[:-1], 'diag_unique_encounters.h5')\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import config\n",
    "\n",
    "# Configure Dask\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,              # Number of workers\n",
    "    threads_per_worker=1,     # Number of threads per worker\n",
    "    memory_limit='7.50GB',       # Memory limit per worker\n",
    "    processes=True,           # Use separate processes for each worker\n",
    "    dashboard_address=':8787' # Dashboard address http://localhost:8787\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.get_versions(check=True)\n",
    "\n",
    "config.set({'distributed.worker.memory.target': 0.50,    # Spill to disk at 60% memory usage\n",
    "            'distributed.worker.memory.spill': 0.60,     # Spill to disk at 70% memory usage\n",
    "            'distributed.worker.memory.pause': 0.80,     # Pause worker at 80% memory usage\n",
    "            'distributed.worker.memory.terminate': 0.95, # Terminate worker at 95% memory usage\n",
    "            'distributed.scheduler.allowed-failures': 4, # Set the allowed failures to 10          \n",
    "           })\n",
    "\n",
    "# Read the data using Dask\n",
    "ddf = dd.read_hdf(input_hdf_path, 'unique_encounters')\n",
    "\n",
    "# Repartition the dataframe into smaller chunks\n",
    "num_partitions = 2\n",
    "ddf = ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    return chunk.drop_duplicates()\n",
    "\n",
    "# Apply the function to each partition\n",
    "result = ddf.map_partitions(process_chunk)\n",
    "\n",
    "# Compute the result\n",
    "computed_result = result.compute()\n",
    "#computed_result.to_csv(os.path.join(working_dir[:-1], 'dup_diag_unique_encounters.csv'), index=False)\n",
    "# Remove duplicates again to ensure global uniqueness\n",
    "computed_result = computed_result.drop_duplicates()\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file\n",
    "computed_result.to_csv(output_csv_path, single_file=True, index=False)\n",
    "\n",
    "print(f\"Combined CSV saved to: {output_csv_path}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to make the Inpatient and Emergency Screens break into chunks - no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Repartition the DataFrames into smaller chunks for better parallelism\n",
    "num_partitions = 16\n",
    "diag_ddf = diag_ddf.repartition(npartitions=num_partitions)\n",
    "vitals_ddf = vitals_ddf.repartition(npartitions=num_partitions)\n",
    "med_ddf = med_ddf.repartition(npartitions=num_partitions)\n",
    "lab_ddf = lab_ddf.repartition(npartitions=num_partitions)\n",
    "proc_ddf = proc_ddf.repartition(npartitions=num_partitions)\n",
    "\n",
    "# Ensure consistent partitioning and indexing\n",
    "def set_index_and_repartition(ddf):\n",
    "    ddf = ddf.set_index('encounter_id').repartition(npartitions=num_partitions)\n",
    "    return ddf\n",
    "\n",
    "diag_ddf = set_index_and_repartition(diag_ddf)\n",
    "vitals_ddf = set_index_and_repartition(vitals_ddf)\n",
    "med_ddf = set_index_and_repartition(med_ddf)\n",
    "lab_ddf = set_index_and_repartition(lab_ddf)\n",
    "proc_ddf = set_index_and_repartition(proc_ddf)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Merge DataFrames sequentially with intermediate persisting\n",
    "merged = diag_ddf.merge(vitals_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(med_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(proc_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "merged = merged.merge(lab_ddf, on='encounter_id', how='inner')\n",
    "merged = merged.persist()\n",
    "\n",
    "# Drop duplicates just in case\n",
    "merged = merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Write the combined DataFrame to a single CSV file directly with Dask\n",
    "merged.to_csv(output_csv_path, single_file=True, index=False)\n",
    "\n",
    "print(f\"Intersection CSV saved to: {output_csv_path}\")\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "cluster.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to clean up client\n",
    "\"\"\"\n",
    "# Start a Dask client with the dashboard\n",
    "print(client)\n",
    "\n",
    "# Open the dashboard URL printed in the output and monitor the tasks\n",
    "# Close the Dask client and cluster\n",
    "client.close()\n",
    "cluster.close()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
